{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963e4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def setup_env():\n",
    "    \n",
    "    env_path = os.path.join(os.getcwd(), '../.env')\n",
    "\n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        \n",
    "        print(f\"Loaded environment variables from: \\033[94m{env_path}\\033[0m\")\n",
    "    else:\n",
    "            print(\"\\033[91mError: .env file not found. Please create one with your OPENAI_API_KEY.\\033[0m\")\n",
    "            sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb9361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. 출처(page, source)를 답변헤 포함하세요. 답변은 한글로 답변해 주세요.\"\"\"\n",
    "\n",
    "human = \"\"\"#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceecd8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\\n검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\\n기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. 출처(page, source)를 답변헤 포함하세요. 답변은 한글로 답변해 주세요.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='#Question: \\n{question} \\n\\n#Context: \\n{context} \\n\\n#Answer:'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455b3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Evaluation\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "As an LLM evaluator (judge), please assess the LLM's response to the given question. Evaluate the response's accuracy, comprehensiveness, and context precision based on the provided context. After your evaluation, return only the numerical scores in the following format:\n",
    "Accuracy: [score]\n",
    "Comprehensiveness: [score]\n",
    "Context Precision: [score]\n",
    "Final: [normalized score]\n",
    "Grading rubric:\n",
    "\n",
    "Accuracy (0-10 points):\n",
    "Evaluate how well the answer aligns with the information provided in the given context.\n",
    "\n",
    "0 points: The answer is completely inaccurate or contradicts the provided context\n",
    "4 points: The answer partially aligns with the context but contains significant inaccuracies\n",
    "7 points: The answer mostly aligns with the context but has minor inaccuracies or omissions\n",
    "10 points: The answer fully aligns with the provided context and is completely accurate\n",
    "\n",
    "\n",
    "Comprehensiveness (0-10 points):\n",
    "\n",
    "0 points: The answer is completely inadequate or irrelevant\n",
    "3 points: The answer is accurate but too brief to fully address the question\n",
    "7 points: The answer covers main aspects but lacks detail or misses minor points\n",
    "10 points: The answer comprehensively covers all aspects of the question\n",
    "\n",
    "\n",
    "Context Precision (0-10 points):\n",
    "Evaluate how precisely the answer uses the information from the provided context.\n",
    "\n",
    "0 points: The answer doesn't use any information from the context or uses it entirely incorrectly\n",
    "4 points: The answer uses some information from the context but with significant misinterpretations\n",
    "7 points: The answer uses most of the relevant context information correctly but with minor misinterpretations\n",
    "10 points: The answer precisely and correctly uses all relevant information from the context\n",
    "\n",
    "\n",
    "Final Normalized Score:\n",
    "Calculate by summing the scores for accuracy, comprehensiveness, and context precision, then dividing by 30 to get a score between 0 and 1.\n",
    "Formula: (Accuracy + Comprehensiveness + Context Precision) / 30\n",
    "\n",
    "#Given question:\n",
    "{question}\n",
    "\n",
    "#LLM's response:\n",
    "{answer}\n",
    "\n",
    "#Provided context:\n",
    "{context}\n",
    "\n",
    "Please evaluate the LLM's response according to the criteria above. \n",
    "\n",
    "In your output, include only the numerical scores for FINAL NORMALIZED SCORE without any additional explanation or reasoning.\n",
    "ex) 0.81\n",
    "\n",
    "#Final Normalized Score(Just the number):\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70b3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic m   eaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {context} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21480fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to the answer. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the answer, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the answer.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {context} \\n\\n Answer: {answer}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f1b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
