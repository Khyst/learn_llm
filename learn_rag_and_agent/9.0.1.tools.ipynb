{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5b5903",
   "metadata": {},
   "source": [
    "## ì‚¬ìš©ì ì •ì˜ ë„êµ¬(Custom Tool)\n",
    "\n",
    "LangChain ì—ì„œ ì œê³µí•˜ëŠ” ë¹ŒíŠ¸ì¸ ë„êµ¬ ì™¸ì—ë„ ì‚¬ìš©ìê°€ ì§ì ‘ ë„êµ¬ë¥¼ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ ìœ„í•´ì„œëŠ” `langchain.tools` ëª¨ë“ˆì—ì„œ ì œê³µí•˜ëŠ” `tool` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "### @tool ë°ì½”ë ˆì´í„°\n",
    "\n",
    "ì´ ë°ì½”ë ˆì´í„°ëŠ” í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ë„êµ¬ì˜ ë™ì‘ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì‚¬ìš© ë°©ë²•**\n",
    "1. í•¨ìˆ˜ ìœ„ì— `@tool` ë°ì½”ë ˆì´í„° ì ìš©\n",
    "2. í•„ìš”ì— ë”°ë¼ ë°ì½”ë ˆì´í„° ë§¤ê°œë³€ìˆ˜ ì„¤ì •\n",
    "\n",
    "ì´ ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ì¼ë°˜ Python í•¨ìˆ˜ë¥¼ ê°•ë ¥í•œ ë„êµ¬ë¡œ ì‰½ê²Œ ë³€í™˜í•  ìˆ˜ ìˆìœ¼ë©°, ìë™í™”ëœ ë¬¸ì„œí™”ì™€ ìœ ì—°í•œ ì¸í„°í˜ì´ìŠ¤ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f6f40",
   "metadata": {},
   "source": [
    "### êµ¬ê¸€ ë‰´ìŠ¤ê¸°ì‚¬ ê²€ìƒ‰ ë„êµ¬\n",
    "\n",
    "`langchain-teddynote` íŒ¨í‚¤ì§€ì—ì„œ ì œê³µí•˜ëŠ” `GoogleNews` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ê¸€ ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "- API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸)\n",
    "\n",
    "news.google.com ì—ì„œ ì œê³µí•˜ëŠ” ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì„¤ëª…**\n",
    "- êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "- í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
    "- `k` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bdf310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from: \u001b[94m/home/ras/0.agent_ai_ws/src/learn_rag_and_agent/learn_rag_and_agent/../.env\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def setup_env():\n",
    "    \n",
    "    env_path = os.path.join(os.getcwd(), '../.env')\n",
    "\n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        \n",
    "        print(f\"Loaded environment variables from: \\033[94m{env_path}\\033[0m\")\n",
    "    else:\n",
    "            print(\"\\033[91mError: .env file not found. Please create one with your OPENAI_API_KEY.\\033[0m\")\n",
    "            sys.exit(1)\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66899bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import GoogleNews\n",
    "\n",
    "# ë„êµ¬ ìƒì„±\n",
    "news_tool = GoogleNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e475bfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://news.google.com/rss/articles/CBMikAFBVV95cUxOYUt1M1FBQmlIOUVKVU5iOVJmWjhQNGlDYUpIQVVFWDdzX0FMYndKU3JFSmprUjNtekdXWGxEb0tNSmx3ZWFzZWlJNXRETGJhY3kzRnRaWFNBSnRVRlRjc3FUWWxBTnJwcW5LUUFwejJvWGgtSE9RbUY4emE1WENZSUFCb0ZaOFdNU0ZRZUhvZEHSAaQBQVVfeXFMTTV5b3N3YWxhcExPUHFEMGhfNkRoZmlJbmZ4dXplNUU4WmlBQ3ZRWnMzTVFTb1JWMVB0dWFwNjBQLU1FODg5NHUzbEh3MWctbnRrWkppX2JldEVLR1JJSF8yVUd3Rll4bThJbjIyQkZpTnE5LV9WWGdzaFhKbXBXbjladnRhU00zbGs1S055dkhTMTNQdmtoV3VseEJFdTFPQ2JrQk8?oc=5',\n",
       "  'content': \"æª¢, â€˜èˆ‡ ì„ ê±°ë²•Â·ê³µìˆ˜ì²˜ë²• ê°•í–‰ ì¶©ëŒ' ë°•ë²”ê³„Â·ë°•ì£¼ë¯¼ ë²Œê¸ˆí˜• êµ¬í˜• - ì¡°ì„ ì¼ë³´\"},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMidEFVX3lxTFBwUmJBeDNvdmhpcTM3ekVpRndzZGE0cW43QUI4TDZXQ2hHZU1WeE01RU9GUVYyWGxla0syd3g3ZHdNMWdjTGdqMVNfU3BYMVNJOUllbGtCYlBMd2duTHdGMENscjdldGxtQmRQNy1fd3h2TGxK?oc=5',\n",
       "  'content': 'ì´ ëŒ€í†µë ¹ ì§€ì§€ìœ¨ 60%â€¦ë¯¼ì£¼ 42%, êµ­í˜ 24%, í˜ì‹ ë‹¹ 3% [ê°¤ëŸ½] - í•œê²¨ë ˆ'},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiWkFVX3lxTE5sQjJSUjVYcVFlUzZoRXhMRjIwUWZfZllab2x4ZzMyVHZpUlBiTHJNVjJKZ2dfb28zcWtxN25CalRVZk11T3N3MnpmNGxaTVY0VlA5OGNfRG5OZ9IBX0FVX3lxTE9LM3h1OERBTXFCd3NaWnhRTy04UkpPNThHVXlpcGlObnVxN3pLMjBPSHVUVXpJQ2N0c2UzaXV6SUJDdDBWT3gwa0FwYlZaU0JwbWQ0N051U0tJYi1iOVpR?oc=5',\n",
       "  'content': 'í•œêµ­, ë¹„í•µë³´ìœ êµ­ ì¤‘ ì„¸ë²ˆì§¸ â€˜í•µì â€™ ê³µì‹ ì¶”ì§„â€¦ë¯¸êµ­ í–‰ì •ë¶€Â·ì˜íšŒÂ·IAEA ë“± ì—¬ëŸ¬ ê´€ë¬¸ ë„˜ì–´ì•¼ - ê²½í–¥ì‹ ë¬¸'},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiiwFBVV95cUxOYkZUT1JwUzN2OGdHWWJoOUo0OGFjS1FySmNLYzV4Tjk2OEtGeU40WkFtNm1EQUJZX0p1WjE0NzNDcmh6a3V2ZnJBNlRnelpnaWloNExSaEwzbGs1dHB1WS1mU3E4YTNFRVZfODI4WmU3aHI0bUp0NUpjdWF2N1NPOUNDaUg0RkVzWXZB?oc=5',\n",
       "  'content': \"ëŒ€í•œë¯¼êµ­ ë…ì ê°œë°œ ìš°ì£¼ ë°œì‚¬ì²´ 'ëˆ„ë¦¬í˜¸' 4ì°¨ ë°œì‚¬ ì„±ê³µ - ì¹´ë“œ/í•œì»· | ë©€í‹°ë¯¸ë””ì–´ - ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘\"},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiZkFVX3lxTE5TRWctcWhTUncyRjQ4VVNTUmhXYlRWSnBTQ2Z0bUY4QTF2Z0pNeV9seENOQllOdUlXRzVrMUhxUXNwOFpJejY5RXVxS3ZhZno2dzBlbHVjY2x2QllzRXdCUVExYTFJZw?oc=5',\n",
       "  'content': 'í™ì½© í™”ì¬ ì‚¬ë§ì 94ëª…â€¦ì–´ë‘  ì† ì§„í™”, êµ¬ì¡° ì´ë ¥ - KBS ë‰´ìŠ¤'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰\n",
    "news_tool.search_latest(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46168c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://news.google.com/rss/articles/CBMiVkFVX3lxTE8zVUw2NG9jQlBxZk01WFZWbFdmQnBfSkRZOWNaeTNLYkJQbURJZWcyVDBuWmJidDZSdEZlN3lZX05jcmVLdUl4WC1tekhaQmpOWE5pM1hR?oc=5',\n",
       "  'content': \"'AI ë¹šíˆ¬' ë‚˜ì„  ì˜¤ë¼í´, ì‚¬ìƒ ìµœê³ ê°€ ì°ê³  ë‘ ë‹¬ë§Œì— ì¶”ë½â€¦ë¬´ìŠ¨ ì¼? - ì§€ë””ë„·ì½”ë¦¬ì•„\"},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiYEFVX3lxTFB2MFpQX3kwWkR0c3IwUE9jZjR3NEhQYm1MZUg1Q29wdTU5NGEtTE11bjk1aXlwMlZvMDlDUk82em8tdjM2WmRJS3p6MTFfMzZxLXl2Y0FGaXVJRG5sNno2Sw?oc=5',\n",
       "  'content': \"í•€íŠ¸, AI íˆ¬ì ì•Œê³ ë¦¬ì¦˜ 'ë¯¸êµ­ ê±°ë²„ë„ŒìŠ¤' ë¶€ê° - ì•„ì‹œì•„ê²½ì œ\"},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMickFVX3lxTE9sNjdKV2l4M3AzNlhtSTJfVUZTUW5FcURkeTE0VTllRldPeXZjakFGX0s5b2RMZUt3NXZsdGd0UFJSenQxdUUxUVgzX0Jqc3g2Mmg2dUlUUEhTWm9ScHhFMGhVSjlkek04Z0w1S1FkUXByUQ?oc=5',\n",
       "  'content': 'ì•„ëŒì½” ì‚°í•˜ VCë“¤ ë‚´ì„¸ì›Œ AI íˆ¬ì ë³¸ê²©í™”í•˜ëŠ” ì‚¬ìš°ë”” - ë§ˆì¼“ì¸'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰\n",
    "news_tool.search_by_keyword(\"AI íˆ¬ì\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afd2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import GoogleNews\n",
    "from langchain.tools import tool\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ ìƒì„±\n",
    "@tool\n",
    "def search_keyword(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Look up news by keyword\"\"\"\n",
    "    print(query)\n",
    "    news_tool = GoogleNews()\n",
    "    return news_tool.search_by_keyword(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664fce9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f906dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain AI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://news.google.com/rss/articles/CBMiZ0FVX3lxTFBHNDF5Y0NnV2ZiUzEyalBkNEJ5ZE9sdENiZkNnMkZ5MnFqUERHTEFiNWdXOWoxRE5wWFl4ZlJPWmJ0Yi1Pdm5LeHNLX0xTdUFNS2dxQ01zMmU1TE9BdWdsZGRaZ2xoc3c?oc=5',\n",
       "  'content': 'í•œì»´, â€˜ì˜¤í”ˆë°ì´í„°ë¡œë” PDFâ€™ ê¸€ë¡œë²Œ AI ê°œë°œ í‘œì¤€ ë“±ë¡ - ì•„ì´í‹°ë°ì¼ë¦¬'},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiZ0FVX3lxTE9kcF8wdlczdXMtX2E1b01KdmMxOWVhNkV2OC1TT3dGdG0xbEtPTWJvQmR4UzFubjJUekNhNjRVcHhUaEJ0R3VZUTNIQUpJWmNybzhqTG1VOTRmcHJhZ2N6X3ZBNnE4ZTg?oc=5',\n",
       "  'content': 'í•œì»´, PDF ë°ì´í„° ì¶”ì¶œ ê¸°ìˆ  ê¸€ë¡œë²Œ AI ê°œë°œ í‘œì¤€ì— ë“±ë¡...ì „ ì„¸ê³„ ìˆ˜ì‹­ë§Œ AI ê°œë°œì ëŒ€ìƒ ê¸°ìˆ  ê³µì‹ ë ¥ í™•ë³´ - ì¸ê³µì§€ëŠ¥ì‹ ë¬¸'},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiakFVX3lxTE80VTcxeGE0WTl2dEhXdExlYTRWNGZrSFRZb2Q0MW93U1VobUZhV25mUjZnN3hvM0F2UHY0TU14RlVMSXJwc3EzX19NSFdRcGVOa0QzU012NDhfQjJ0NVhCSl9FRmNLdVRSdWc?oc=5',\n",
       "  'content': \"í•œì»´, PDF ë°ì´í„° ì¶”ì¶œ ê¸°ìˆ  â€˜ë­ì²´ì¸' ê¸€ë¡œë²Œ AI ê°œë°œ í‘œì¤€ì— ë“±ë¡ - AIíƒ€ì„ìŠ¤\"},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMibkFVX3lxTE1qMC1EYjRZOEpMNVBEVVpSNm1BX3A4a3FrdmEyOTdDNWdjMGR1ZEgzWlpHZnRFZ1pSUXdKa3JnOXBQZU5qU3dKS3Q5VUNwLVZxRFZmbG1SZzJVSUoyM2lJbnZRbzhYYUJJcnFHWmp3?oc=5',\n",
       "  'content': 'ë­ì²´ì¸ LangChain ì´ë€ ë¬´ì—‡ì¸ê°€? | ì¸ì‚¬ì´íŠ¸ë¦¬í¬íŠ¸ | ì‚¼ì„±SDS - Samsung SDS'},\n",
       " {'url': 'https://news.google.com/rss/articles/CBMiT0FVX3lxTFBZRktKTC13Z0RJblROV3RnMnlBUkJHUGJFNHp4dDk2cE9fSGlBSEMyU21obzZhaE00RDJib0d2WHFNMEpQVVdNSlhtdm4zMUU?oc=5',\n",
       "  'content': 'AI ì—ì´ì „íŠ¸ ê°œë°œ í”Œë«í¼ â€˜ë­ì²´ì¸â€™, 1ì–µ 2,500ë§Œ ë‹¬ëŸ¬ íˆ¬ì ìœ ì¹˜ë¡œ ìœ ë‹ˆì½˜ ë“±ê·¹ - ì™€ìš°í…Œì¼'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‹¤í–‰ ê²°ê³¼\n",
    "search_keyword.invoke({\"query\": \"LangChain AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df17564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# ë„êµ¬ ìƒì„±\n",
    "tool = TavilySearchResults(\n",
    "    max_results=6,\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    # include_images=True,\n",
    "    # search_depth=\"advanced\", # or \"basic\"\n",
    "    include_domains=[\"github.io\", \"wikidocs.net\"],\n",
    "    # exclude_domains = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4986b73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '01. ë„êµ¬(Tools) - <ë­ì²´ì¸LangChain ë…¸íŠ¸>',\n",
       "  'url': 'https://wikidocs.net/262582',\n",
       "  'content': '```\\n# ë„êµ¬ ì‹¤í–‰ tool.invoke({\"query\": \"LangChain Tools ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”\"}) \\n``` [...] Published with WikiDocs\\n\\n1. <ë­ì²´ì¸LangChain ë…¸íŠ¸> - Langâ€¦\\n2. CH16 ì—ì´ì „íŠ¸(Agent)\\n3. 01. ë„êµ¬(Tools)\\n\\n1. ë„ì„œ ì¦ì • ì´ë²¤íŠ¸ !!\\n2. ìœ„í‚¤ë…ìŠ¤\\n\\n# 01. ë„êµ¬(Tools)\\n\\nê´‘ê³ ê°€ ì¶œë ¥ë  ìœ„ì¹˜ì…ë‹ˆë‹¤.\\n\\nê´‘ê³ ê°€ ì¶œë ¥ë  ìœ„ì¹˜ì…ë‹ˆë‹¤.\\n\\n# ë„êµ¬ (Tools)\\n\\në„êµ¬(Tool)ëŠ” ì—ì´ì „íŠ¸, ì²´ì¸ ë˜ëŠ” LLMì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\\n\\nLangChain ì—ì„œ ê¸°ë³¸ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ì ì •ì˜ ë„êµ¬(Custom Tool) ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n\\nLangChain ì— í†µí•©ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ëŠ” ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n LangChain í†µí•©ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸\\n\\n```\\n# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼ from dotenv import load_dotenv # API í‚¤ ì •ë³´ ë¡œë“œ load_dotenv() \\n``` [...] [ {\\'url\\': \\' \\'content\\': \\'íƒœê·¸:\\\\nAgent,\\\\nAPI KEY,\\\\nFAISS,\\\\nLangChain,\\\\nLangSmith,\\\\nmemory,\\\\nOpenAI,\\\\nPython,\\\\nRetriever,\\\\nTavily Search,\\\\ntools,\\\\nê²€ìƒ‰ë„êµ¬,\\\\në­ì²´ì¸,\\\\nì—ì´ì „íŠ¸\\\\nì¹´í…Œê³ ë¦¬:\\\\nlangchain\\\\nì—…ë°ì´íŠ¸: 2024ë…„ 02ì›” 09ì¼\\\\nì°¸ê³ \\\\nLangChain RAG íŒŒí—¤ì¹˜ê¸°: ë‹¤ìŒì˜ ì¶”ì  ë§í¬ì—ì„œ ìì„¸í•œ ë‹¨ê³„ë³„ ìˆ˜í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\\\\nLangSmith ì¶”ì \\\\në‹¤ìŒì˜ ì¶”ì  ë§í¬ì—ì„œ ìì„¸í•œ ë‹¨ê³„ë³„ ìˆ˜í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\\\\nLangSmith ì¶”ì \\\\nâ‘£ ë©”ëª¨ë¦¬ ì¶”ê°€í•˜ê¸°\\\\nì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, ì´ ì—ì´ì „íŠ¸ëŠ” ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤. LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼\\\\në°”ë¡œê°€ê¸° ğŸ‘€\\\\n[LangChain] ì—ì´ì „íŠ¸(Agent)ì™€ ë„êµ¬(tools)ë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ\\\\n2024ë…„ 02ì›” 09ì¼\\\\n41 ë¶„ ì†Œìš”\\\\nì´ ê¸€ì—ì„œëŠ” LangChain ì˜',\n",
       "  'score': 0.8533396,\n",
       "  'raw_content': '[**<ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡·**](/book/14314)   [CH01 LangChain ì‹œì‘í•˜ê¸°](javascript:page(233341))   [01. ì„¤ì¹˜ ì˜ìƒë³´ê³  ë”°ë¼í•˜ê¸°](javascript:page(257836))   [02. OpenAI API í‚¤ ë°œê¸‰ ë° í…ŒìŠ¤íŠ¸](javascript:page(233342))   [03. LangSmith ì¶”ì  ì„¤ì •](javascript:page(250954))   [04. OpenAI API ì‚¬ìš©(GPT-4o ë©€í‹°ëª¨ë‹¬)](javascript:page(233343))   [05. LangChain Expression Language(LCEL)](javascript:page(233344))   [06. LCEL ì¸í„°í˜ì´ìŠ¤](javascript:page(233345))   [07. Runnable](javascript:page(233346))   [CH02 í”„ë¡¬í”„íŠ¸(Prompt)](javascript:page(233347))   [01. í”„ë¡¬í”„íŠ¸(Prompt)](javascript:page(233351))   [02. í“¨ìƒ· í”„ë¡¬í”„íŠ¸(FewShotPromptTemplate)](javascript:page(233348))   [03. LangChain Hub](javascript:page(233349))   [04. ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸(Hubì— ì—…ë¡œë“œ)](javascript:page(233350))   [CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)](javascript:page(233771))   [01. Pydantic ì¶œë ¥ íŒŒì„œ(PydanticOutputParser)](javascript:page(233786))   [02. ì½¤ë§ˆ êµ¬ë¶„ì ì¶œë ¥ íŒŒì„œ(CommaSeparatedListOutputParser)](javascript:page(233787))   [03. êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì„œ(StructuredOuputParser)](javascript:page(233788))   [04. JSON ì¶œë ¥ íŒŒì„œ(JsonOutputParser)](javascript:page(233789))   [05. ë°ì´í„°í”„ë ˆì„ ì¶œë ¥ íŒŒì„œ(PandasDataFrameOutputParser)](javascript:page(233790))   [06. ë‚ ì§œ í˜•ì‹ ì¶œë ¥ íŒŒì„œ(DatetimeOutputParser)](javascript:page(233791))   [07. ì—´ê±°í˜• ì¶œë ¥ íŒŒì„œ(EnumOutputParser)](javascript:page(233792))   [08. ì¶œë ¥ ìˆ˜ì • íŒŒì„œ(OutputFixingParser)](javascript:page(233793))   [CH04 ëª¨ë¸(Model)](javascript:page(233772))   [01. ë‹¤ì–‘í•œ LLM ëª¨ë¸ í™œìš©](javascript:page(233795))   [02. ìºì‹±(Cache)](javascript:page(233796))   [03. ëª¨ë¸ ì§ë ¬í™”(Serialization) - ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°](javascript:page(233798))   [04. í† í° ì‚¬ìš©ëŸ‰ í™•ì¸](javascript:page(233797))   [05. êµ¬ê¸€ ìƒì„± AI(Google Generative AI)](javascript:page(233799))   [06. í—ˆê¹…í˜ì´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸(HuggingFace Endpoints)](javascript:page(233802))   [07. í—ˆê¹…í˜ì´ìŠ¤ ë¡œì»¬(HuggingFace Local)](javascript:page(233803))   [08. í—ˆê¹…í˜ì´ìŠ¤ íŒŒì´í”„ë¼ì¸(HuggingFace Pipeline)](javascript:page(233804))   [09. ì˜¬ë¼ë§ˆ(Ollama)](javascript:page(233805))   [10. GPT4ALL](javascript:page(233806))   [11. ë¹„ë””ì˜¤(Video) ì§ˆì˜ ì‘ë‹µ LLM (Gemini)](javascript:page(263314))   [CH05 ë©”ëª¨ë¦¬(Memory)](javascript:page(233773))   [01. ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬(ConversationBufferMemory)](javascript:page(233801))   [02. ëŒ€í™” ë²„í¼ ìœˆë„ìš° ë©”ëª¨ë¦¬(ConversationBufferWindowMemory)](javascript:page(233800))   [03. ëŒ€í™” í† í° ë²„í¼ ë©”ëª¨ë¦¬(ConversationTokenBufferMemory)](javascript:page(233807))   [04. ëŒ€í™” ì—”í‹°í‹° ë©”ëª¨ë¦¬(ConversationEntityMemory)](javascript:page(233808))   [05. ëŒ€í™” ì§€ì‹ê·¸ë˜í”„ ë©”ëª¨ë¦¬(ConversationKGMemory)](javascript:page(233809))   [06. ëŒ€í™” ìš”ì•½ ë©”ëª¨ë¦¬(ConversationSummaryMemory)](javascript:page(233810))   [07. ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ ë©”ëª¨ë¦¬(VectorStoreRetrieverMemory)](javascript:page(233811))   [08. LCEL Chain ì— ë©”ëª¨ë¦¬ ì¶”ê°€](javascript:page(233812))   [09. SQLite ì— ëŒ€í™”ë‚´ìš© ì €ì¥](javascript:page(233813))   [10. RunnableWithMessageHistoryì— ChatMessageHistoryì¶”ê°€](javascript:page(254682))   [CH06 ë¬¸ì„œ ë¡œë”(Document Loader)](javascript:page(233775))   [01. ë„íë¨¼íŠ¸(Document) ì˜ êµ¬ì¡°](javascript:page(253706))   [02. PDF](javascript:page(253707))   [03. í•œê¸€(HWP)](javascript:page(253708))   [04. CSV](javascript:page(253709))   [05. Excel](javascript:page(253710))   [06. Word](javascript:page(253711))   [07. PowerPoint](javascript:page(253712))   [08. ì›¹ ë¬¸ì„œ(WebBaseLoader)](javascript:page(253713))   [09. í…ìŠ¤íŠ¸(TextLoader)](javascript:page(253714))   [10. JSON](javascript:page(253715))   [11. Arxiv](javascript:page(253716))   [12. UpstageLayoutAnalysisLoader](javascript:page(253717))   [13. LlamaParser](javascript:page(253718))   [CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)](javascript:page(233776))   [01. ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (CharacterTextSplitter)](javascript:page(233998))   [02. ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (RecursiveCharacterTextSplitter)](javascript:page(233999))   [03. í† í° í…ìŠ¤íŠ¸ ë¶„í• (TokenTextSplitter)](javascript:page(234002))   [04. ì‹œë©˜í‹± ì²­ì»¤(SemanticChunker)](javascript:page(234003))   [05. ì½”ë“œ ë¶„í• (Python, Markdown, JAVA, C++, C#, GO, JS, Latex ë“±)](javascript:page(234004))   [06. ë§ˆí¬ë‹¤ìš´ í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (MarkdownHeaderTextSplitter)](javascript:page(234005))   [07. HTML í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (HTMLHeaderTextSplitter)](javascript:page(234006))   [08. ì¬ê·€ì  JSON ë¶„í• (RecursiveJsonSplitter)](javascript:page(234007))   [CH08 ì„ë² ë”©(Embedding)](javascript:page(233777))   [01. OpenAIEmbeddings](javascript:page(233815))   [02. ìºì‹œ ì„ë² ë”©(CacheBackedEmbeddings)](javascript:page(233816))   [03. í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©(HuggingFace Embeddings)](javascript:page(233817))   [04. UpstageEmbeddings](javascript:page(253106))   [05. OllamaEmbeddings](javascript:page(253107))   [06. GPT4ALL ì„ë² ë”©](javascript:page(233818))   [07. Llama CPP ì„ë² ë”©](javascript:page(233819))   [CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)](javascript:page(233778))   [01. Chroma](javascript:page(234094))   [02. FAISS](javascript:page(234014))   [03. Pinecone](javascript:page(252407))   [CH10 ê²€ìƒ‰ê¸°(Retriever)](javascript:page(233779))   [01. ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ê²€ìƒ‰ê¸°(VectorStore-backed Retriever)](javascript:page(234016))   [02. ë¬¸ë§¥ ì••ì¶• ê²€ìƒ‰ê¸°(ContextualCompressionRetriever)](javascript:page(234097))   [03. ì•™ìƒë¸” ê²€ìƒ‰ê¸°(EnsembleRetriever)](javascript:page(234100))   [04. ê¸´ ë¬¸ë§¥ ì¬ì •ë ¬(LongContextReorder)](javascript:page(234101))   [05. ìƒìœ„ ë¬¸ì„œ ê²€ìƒ‰ê¸°(ParentDocumentRetriever)](javascript:page(234164))   [06. ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°(MultiQueryRetriever)](javascript:page(234109))   [07. ë‹¤ì¤‘ ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°(MultiVectorRetriever)](javascript:page(234281))   [08. ì…€í”„ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°(SelfQueryRetriever)](javascript:page(234475))   [09. ì‹œê°„ ê°€ì¤‘ ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°(TimeWeightedVectorStoreRetriever)](javascript:page(234604))   [10. í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸°(Kiwi, Kkma, Okt) + BM25 ê²€ìƒ‰ê¸°](javascript:page(251980))   [11. Convex Combination(CC) ì ìš©ëœ ì•™ìƒë¸” ê²€ìƒ‰ê¸°(EnsembleRetriever)](javascript:page(263833))   [CH11 ë¦¬ë­ì»¤(Reranker)](javascript:page(253434))   [01. Cross Encoder Reranker](javascript:page(253836))   [02. Cohere Reranker](javascript:page(253837))   [03. Jina Reranker](javascript:page(253838))   [04. FlashRank Reranker](javascript:page(253839))   [CH12 Retrieval Augmented Generation(RAG)](javascript:page(233780))   [01. PDF ë¬¸ì„œ ê¸°ë°˜ QA(Question-Answer)](javascript:page(251190))   [02. ë„¤ì´ë²„ ë‰´ìŠ¤ê¸°ì‚¬ QA(Question-Answer)](javascript:page(234008))   [03. RAG ì˜ ê¸°ëŠ¥ë³„ ë‹¤ì–‘í•œ ëª¨ë“ˆ í™œìš©ê¸°](javascript:page(234009))   [04. RAPTOR: ê¸´ ë¬¸ë§¥ ìš”ì•½(Long Context Summary)](javascript:page(234017))   [05. ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” RAG ì²´ì¸](javascript:page(252858))   [CH13 LangChain Expression Language(LCEL)](javascript:page(233781))   [01. RunnablePassthrough](javascript:page(235580))   [02. Runnable êµ¬ì¡°(ê·¸ë˜í”„) ê²€í† ](javascript:page(235884))   [03. RunnableLambda](javascript:page(235705))   [04. LLM ì²´ì¸ ë¼ìš°íŒ…(RunnableLambda, RunnableBranch)](javascript:page(235882))   [05. RunnableParallel](javascript:page(235883))   [06. ë™ì  ì†ì„± ì§€ì •(configurable\\\\_fields, configurable\\\\_alternatives)](javascript:page(235704))   [07. @chain ë°ì½”ë ˆì´í„°ë¡œ Runnable êµ¬ì„±](javascript:page(235703))   [08. RunnableWithMessageHistory](javascript:page(235581))   [09. ì‚¬ìš©ì ì •ì˜ ì œë„¤ë ˆì´í„°(generator)](javascript:page(235885))   [10. Runtime Arguments ë°”ì¸ë”©](javascript:page(235886))   [11. í´ë°±(fallback) ëª¨ë¸ ì§€ì •](javascript:page(235938))   [CH14 ì²´ì¸(Chains)](javascript:page(233774))   [01. ë¬¸ì„œ ìš”ì•½](javascript:page(234020))   [02. SQL](javascript:page(234019))   [03. êµ¬ì¡°í™”ëœ ì¶œë ¥ ì²´ì¸(with\\\\_structered\\\\_output)](javascript:page(256983))   [CH15 í‰ê°€(Evaluations)](javascript:page(259203))   [01. í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±(RAGAS)](javascript:page(259204))   [02. RAGAS ë¥¼ í™œìš©í•œ í‰ê°€](javascript:page(259205))   [03. ìƒì„±í•œ í‰ê°€ìš© ë°ì´í„°ì…‹ ì—…ë¡œë“œ(HuggingFace Dataset)](javascript:page(259206))   [04. LangSmith ë°ì´í„°ì…‹ ìƒì„±](javascript:page(259207))   [05. LLM-as-Judge](javascript:page(259208))   [06. ì„ë² ë”© ê¸°ë°˜ í‰ê°€(embedding\\\\_distance)](javascript:page(259210))   [07. ì‚¬ìš©ì ì •ì˜(Custom) LLM í‰ê°€](javascript:page(259212))   [08. Rouge, BLEU, METEOR, SemScore ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± í‰ê°€](javascript:page(259213))   [09. ì‹¤í—˜(Experiment) í‰ê°€ ë¹„êµ](javascript:page(259214))   [10. ìš”ì•½(Summary) ë°©ì‹ì˜ í‰ê°€](javascript:page(259215))   [11. Groundedness(í• ë£¨ì‹œë„¤ì´ì…˜) í‰ê°€](javascript:page(259216))   [12. ì‹¤í—˜ ë¹„êµ(Pairwise Evaluation)](javascript:page(259217))   [13. ë°˜ë³µ í‰ê°€](javascript:page(259218))   [14. ì˜¨ë¼ì¸ í‰ê°€ë¥¼ í™œìš©í•œ í‰ê°€ ìë™í™”](javascript:page(259219))   [CH16 ì—ì´ì „íŠ¸(Agent)](javascript:page(233782))   [01. ë„êµ¬(Tools)](javascript:page(262582))   [02. ë„êµ¬ ë°”ì¸ë”©(Binding Tools)](javascript:page(262585))   [03. ì—ì´ì „íŠ¸(Agent)](javascript:page(262586))   [04. Claude, Gemini, Ollama, Together.ai ë¥¼ í™œìš©í•œ Agent](javascript:page(262592))   [05. Iteration ê¸°ëŠ¥ê³¼ ì‚¬ëŒ ê°œì…(Human-in-the-loop)](javascript:page(262593))   [06. Agentic RAG](javascript:page(262595))   [07. CSVExcel ë°ì´í„° ë¶„ì„ Agent](javascript:page(262597))   [08. Toolkits í™œìš© Agent](javascript:page(262604))   [09. RAG + Image Generator Agent(ë³´ê³ ì„œ ì‘ì„±)](javascript:page(262612))   [10. ë„êµ¬ë¥¼ í™œìš©í•œ í† ë¡  ì—ì´ì „íŠ¸(Two Agent Debates with Tools)](javascript:page(234162))   [CH17 LangGraph](javascript:page(233785))   [01. í•µì‹¬ ê¸°ëŠ¥](javascript:page(265670))   [01. LangGraph ì— ìì£¼ ë“±ì¥í•˜ëŠ” Python ë¬¸ë²•ì´í•´](javascript:page(264613))   [02. LangGraphë¥¼ í™œìš©í•œ ì±—ë´‡ êµ¬ì¶•](javascript:page(264614))   [03. LangGraphë¥¼ í™œìš©í•œ Agent êµ¬ì¶•](javascript:page(264624))   [04. Agent ì— ë©”ëª¨ë¦¬(memory) ì¶”ê°€](javascript:page(265658))   [05. ë…¸ë“œì˜ ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥](javascript:page(265659))   [06. Human-in-the-loop(ì‚¬ëŒì˜ ê°œì…)](javascript:page(265663))   [07. ì¤‘ê°„ë‹¨ê³„ ê°œì… ë˜ëŒë¦¼ì„ í†µí•œ ìƒíƒœ ìˆ˜ì •ê³¼ Replay](javascript:page(265723))   [08. ì‚¬ëŒ(Human)ì—ê²Œ ë¬¼ì–´ë³´ëŠ” ë…¸ë“œ ì¶”ê°€](javascript:page(265737))   [09. ë©”ì‹œì§€ ì‚­ì œ(RemoveMessage)](javascript:page(265749))   [10. ToolNode ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²•](javascript:page(265763))   [11. ë³‘ë ¬ ë…¸ë“œ ì‹¤í–‰ì„ ìœ„í•œ ë¶„ê¸° ìƒì„± ë°©ë²•](javascript:page(265766))   [12. ëŒ€í™” ê¸°ë¡ ìš”ì•½ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•](javascript:page(265767))   [13. ì„œë¸Œê·¸ë˜í”„ ì¶”ê°€ ë° ì‚¬ìš© ë°©ë²•](javascript:page(265768))   [14. ì„œë¸Œê·¸ë˜í”„ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë³€í™˜í•˜ëŠ” ë°©ë²•](javascript:page(265769))   [15. LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ](javascript:page(265770))   [02. êµ¬ì¡° ì„¤ê³„](javascript:page(267807))   [01. ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„±](javascript:page(267808))   [02. Naive RAG](javascript:page(267809))   [03. ê´€ë ¨ì„± ì²´ì»¤(Relevance Checker) ëª¨ë“ˆ ì¶”ê°€](javascript:page(267810))   [04. ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ ì¶”ê°€](javascript:page(267811))   [05. ì¿¼ë¦¬ ì¬ì‘ì„± ëª¨ë“ˆ ì¶”ê°€](javascript:page(267812))   [06. Agentic RAG](javascript:page(267813))   [07. Adaptive RAG](javascript:page(267814))   [03. Use Cases](javascript:page(267815))   [01. ì—ì´ì „íŠ¸ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜ (ê³ ê° ì‘ëŒ€ ì‹œë‚˜ë¦¬ì˜¤)](javascript:page(267816))   [02. ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì—ì´ì „íŠ¸](javascript:page(267817))   [03. CRAG(Corrective RAG)](javascript:page(270686))   [04. Self-RAG](javascript:page(270687))   [05. ê³„íš í›„ ì‹¤í–‰(Plan-and-Execute)](javascript:page(270688))   [06. ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… ë„¤íŠ¸ì›Œí¬(Multi-Agent Collaboration Network)](javascript:page(270689))   [07. ë©€í‹° ì—ì´ì „íŠ¸ ê°ë…ì(Multi-Agent Supervisor)](javascript:page(270690))   [08. ê³„ì¸µì  ë©€í‹° ì—ì´ì „íŠ¸ íŒ€(Hierarchical Multi-Agent Teams)](javascript:page(270691))   [09. SQL ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì—ì´ì „íŠ¸](javascript:page(270692))   [10. STORM ê°œë…ì„ ë„ì…í•œ ì—°êµ¬ë¥¼ ìœ„í•œ ë©€í‹° ì—ì´ì „íŠ¸](javascript:page(270693))   [CH18 ê¸°íƒ€ ì •ë³´](javascript:page(265575))   [01. StreamEvent íƒ€ì…ë³„ ì •ë¦¬](javascript:page(265576))\\n\\n[Published with WikiDocs](/)\\n\\n1. [**<ë­ì²´ì¸LangChain ë…¸íŠ¸> - Langâ€¦**](/book/14314)\\n2. [CH16 ì—ì´ì „íŠ¸(Agent)](/233782)\\n3. [01. ë„êµ¬(Tools)](/262582)\\n\\n1. [ë„ì„œ ì¦ì • ì´ë²¤íŠ¸ !!](/305591)\\n2. [ìœ„í‚¤ë…ìŠ¤](/)\\n\\n# 01. ë„êµ¬(Tools)\\n\\nê´‘ê³ ê°€ ì¶œë ¥ë  ìœ„ì¹˜ì…ë‹ˆë‹¤.\\n\\nê´‘ê³ ê°€ ì¶œë ¥ë  ìœ„ì¹˜ì…ë‹ˆë‹¤.\\n\\n# ë„êµ¬ (Tools)\\n\\në„êµ¬(Tool)ëŠ” ì—ì´ì „íŠ¸, ì²´ì¸ ë˜ëŠ” LLMì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\\n\\nLangChain ì—ì„œ ê¸°ë³¸ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ì ì •ì˜ ë„êµ¬(Custom Tool) ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n\\n**LangChain ì— í†µí•©ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ëŠ” ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\\n\\n* [LangChain í†µí•©ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸](https://python.langchain.com/v0.1/docs/integrations/tools/)\\n\\n```\\n# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼ from dotenv import load_dotenv # API í‚¤ ì •ë³´ ë¡œë“œ load_dotenv() \\n```\\n\\n```\\nTrue\\n```\\n\\n```\\n# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com # !pip install -qU langchain-teddynote from langchain_teddynote import logging # í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤. logging.langsmith(\"CH15-Tools\") \\n```\\n\\n```\\nLangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤. [í”„ë¡œì íŠ¸ëª…] CH15-Tools \\n```\\n\\n```\\nimport warnings # ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ warnings.filterwarnings(\"ignore\") \\n```\\n\\n## ë¹ŒíŠ¸ì¸ ë„êµ¬(built-in tools)\\n\\në­ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ë„êµ¬(tool) ì™€ íˆ´í‚·(toolkit) ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\ntool ì€ ë‹¨ì¼ ë„êµ¬ë¥¼ ì˜ë¯¸í•˜ë©°, toolkit ì€ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nê´€ë ¨ ë„êµ¬ëŠ” ì•„ë˜ì˜ ë§í¬ì—ì„œ ì°¸ê³ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**ì°¸ê³ **\\n\\n* [LangChain Tools/Toolkits](https://python.langchain.com/docs/integrations/tools/)\\n\\n### Python REPL ë„êµ¬\\n\\nì´ ë„êµ¬ëŠ” Python ì½”ë“œë¥¼ REPL(Read-Eval-Print Loop) í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì£¼ìš” í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤ - PythonREPLTool\\n\\n**ì„¤ëª…**\\n\\n* Python ì…¸ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.\\n* ìœ íš¨í•œ Python ëª…ë ¹ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì‹¤í–‰í•©ë‹ˆë‹¤.\\n* ê²°ê³¼ë¥¼ ë³´ë ¤ë©´ print(...) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\\n\\n**ì£¼ìš” íŠ¹ì§•**\\n\\n* sanitize\\\\_input: ì…ë ¥ì„ ì •ì œí•˜ëŠ” ì˜µì…˜ (ê¸°ë³¸ê°’: True)\\n* python\\\\_repl: PythonREPL ì¸ìŠ¤í„´ìŠ¤ (ê¸°ë³¸ê°’: ì „ì—­ ë²”ìœ„ì—ì„œ ì‹¤í–‰)\\n\\n**ì‚¬ìš© ë°©ë²•**\\n\\n* PythonREPLTool ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\\n* run ë˜ëŠ” arun, invoke ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ Python ì½”ë“œ ì‹¤í–‰\\n\\n**ì…ë ¥ ì •ì œ**\\n\\n* ì…ë ¥ ë¬¸ìì—´ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±, ë°±í‹±, \\'python\\' í‚¤ì›Œë“œ ë“±ì„ ì œê±°í•©ë‹ˆë‹¤.\\n\\n```\\nfrom langchain_experimental.tools import PythonREPLTool # íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. python_tool = PythonREPLTool() \\n```\\n\\n```\\n# íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. print(python_tool.invoke(\"print(100 + 200)\")) \\n```\\n\\n```\\n300\\n```\\n\\nì•„ë˜ëŠ” LLM ì—ê²Œ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ë„ë¡ ìš”ì²­í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n**íë¦„ ì •ë¦¬**\\n\\n1. LLM ëª¨ë¸ì—ê²Œ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” Python ì½”ë“œë¥¼ ì‘ì„±í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.\\n2. ì‘ì„±ëœ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.\\n3. ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\\n\\n```\\nfrom langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnableLambda # íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ì¤‘ê°„ ê³¼ì •ì„ ì¶œë ¥í•˜ê³  ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ def print_and_execute(code, debug=True): if debug: print(\"CODE:\") print(code) return python_tool.invoke(code) # íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You are Raymond Hetting, an expert python programmer, well versed in meta-programming and elegant, concise and short but well documented code. You follow the PEP8 style guide. \" \"Return only the code, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the code.\", ), (\"human\", \"{input}\"), ] ) # LLM ëª¨ë¸ ìƒì„± llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) # í”„ë¡¬í”„íŠ¸ì™€ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ ìƒì„± chain = prompt | llm | StrOutputParser() | RunnableLambda(print_and_execute) \\n```\\n\\n```\\n# ê²°ê³¼ ì¶œë ¥ print(chain.invoke(\"ë¡œë˜ ë²ˆí˜¸ ìƒì„±ê¸°ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\")) \\n```\\n\\n```\\nCODE: import random def generate_lotto_numbers(): return sorted(random.sample(range(1, 46), 6)) print(generate_lotto_numbers()) [14, 17, 18, 22, 23, 37] \\n```\\n\\n### ê²€ìƒ‰ API ë„êµ¬\\n\\nTavily ê²€ìƒ‰ APIë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ë‘ ê°€ì§€ ì£¼ìš” í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤: `TavilySearchResults`ì™€ `TavilyAnswer`.\\n\\n**API í‚¤ ë°œê¸‰ ì£¼ì†Œ**\\n\\n* https://app.tavily.com/\\n\\në°œê¸‰í•œ API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.\\n\\n`.env` íŒŒì¼ì— ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.\\n\\n```\\nTAVILY_API_KEY=tvly-abcdefghijklmnopqrstuvwxyz \\n```\\n\\n### TavilySearchResults\\n\\n**ì„¤ëª…**\\n\\n* Tavily ê²€ìƒ‰ APIë¥¼ ì¿¼ë¦¬í•˜ê³  JSON í˜•ì‹ì˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\\n* í¬ê´„ì ì´ê³  ì •í™•í•˜ë©° ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ì— ìµœì í™”ëœ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\\n* í˜„ì¬ ì´ë²¤íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\\n\\n**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\\n\\n* `max_results` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)\\n* `search_depth` (str): ê²€ìƒ‰ ê¹Šì´ (\"basic\" ë˜ëŠ” \"advanced\")\\n* `include_domains` (List[str]): ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨í•  ë„ë©”ì¸ ëª©ë¡\\n* `exclude_domains` (List[str]): ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œì™¸í•  ë„ë©”ì¸ ëª©ë¡\\n* `include_answer` (bool): ì›ë³¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì§§ì€ ë‹µë³€ í¬í•¨ ì—¬ë¶€\\n* `include_raw_content` (bool): ê° ì‚¬ì´íŠ¸ì˜ ì •ì œëœ HTML ì½˜í…ì¸  í¬í•¨ ì—¬ë¶€\\n* `include_images` (bool): ì¿¼ë¦¬ ê´€ë ¨ ì´ë¯¸ì§€ ëª©ë¡ í¬í•¨ ì—¬ë¶€\\n\\n**ë°˜í™˜ ê°’**\\n\\n* ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” JSON í˜•ì‹ì˜ ë¬¸ìì—´(url, content) í˜¹ì€ ì•„ë˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\\n\\n```\\n# import os # os.environ[\"TAVILY_API_KEY\"] = \"TAVILY API í‚¤ ì…ë ¥\" \\n```\\n\\n```\\nfrom langchain_community.tools.tavily_search import TavilySearchResults # ë„êµ¬ ìƒì„± tool = TavilySearchResults( max_results=6, include_answer=True, include_raw_content=True, # include_images=True, # search_depth=\"advanced\", # or \"basic\" include_domains=[\"github.io\", \"wikidocs.net\"], # exclude_domains = [] ) \\n```\\n\\n```\\n# ë„êµ¬ ì‹¤í–‰ tool.invoke({\"query\": \"LangChain Tools ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”\"}) \\n```\\n\\n```\\n[ {\\'url\\': \\'https://teddylee777.github.io/langchain/langchain-agent/\\', \\'content\\': \\'íƒœê·¸:\\\\nAgent,\\\\nAPI KEY,\\\\nFAISS,\\\\nLangChain,\\\\nLangSmith,\\\\nmemory,\\\\nOpenAI,\\\\nPython,\\\\nRetriever,\\\\nTavily Search,\\\\ntools,\\\\nê²€ìƒ‰ë„êµ¬,\\\\në­ì²´ì¸,\\\\nì—ì´ì „íŠ¸\\\\nì¹´í…Œê³ ë¦¬:\\\\nlangchain\\\\nì—…ë°ì´íŠ¸: 2024ë…„ 02ì›” 09ì¼\\\\nì°¸ê³ \\\\nLangChain RAG íŒŒí—¤ì¹˜ê¸°: ë‹¤ìŒì˜ ì¶”ì  ë§í¬ì—ì„œ ìì„¸í•œ ë‹¨ê³„ë³„ ìˆ˜í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\\\\nLangSmith ì¶”ì \\\\në‹¤ìŒì˜ ì¶”ì  ë§í¬ì—ì„œ ìì„¸í•œ ë‹¨ê³„ë³„ ìˆ˜í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\\\\nLangSmith ì¶”ì \\\\nâ‘£ ë©”ëª¨ë¦¬ ì¶”ê°€í•˜ê¸°\\\\nì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, ì´ ì—ì´ì „íŠ¸ëŠ” ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤. LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼\\\\në°”ë¡œê°€ê¸° ğŸ‘€\\\\n[LangChain] ì—ì´ì „íŠ¸(Agent)ì™€ ë„êµ¬(tools)ë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ\\\\n2024ë…„ 02ì›” 09ì¼\\\\n41 ë¶„ ì†Œìš”\\\\nì´ ê¸€ì—ì„œëŠ” LangChain ì˜ Agent í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ê²€ìƒ‰ê³¼ ğŸ“ ì „ì²´ í…œí”Œë¦¿ ì½”ë“œ\\\\në‹¤ìŒì˜ ì¶”ì  ë§í¬ì—ì„œ ìì„¸í•œ ë‹¨ê³„ë³„ ìˆ˜í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\\\\nLangSmith ì¶”ì \\\\në§ˆë¬´ë¦¬ì…ë‹ˆë‹¤!\\\\n ë¬¸ì„œ ê¸°ë°˜ QA ì‹œìŠ¤í…œ ì„¤ê³„ ë°©ë²• - ì‹¬í™”í¸\\\\n2024ë…„ 02ì›” 06ì¼\\\\n22 ë¶„ ì†Œìš”\\\\nLangChainì˜ RAG ì‹œìŠ¤í…œì„ í†µí•´ ë¬¸ì„œ(PDF, txt, ì›¹í˜ì´ì§€ ë“±)ì— ëŒ€í•œ ì§ˆë¬¸-ë‹µë³€ì„ ì°¾ëŠ” ê³¼ì •ì„ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.\\\\n\\'}, {\\'url\\': \\'https://13akstjq.github.io/TIL/post/2024-07-09-LLMStudyDiaryComprehensiveReviewofLangChainPart4\\', \\'content\\': \\'Memory. ì›ë˜ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤. Langchainì˜ ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ ìœ í˜• ëª©ë¡ì— ëŒ€í•œ ë§í¬ê°€ ê¹¨ì ¸ ìˆì–´ ì°¾ì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ... tools=toolkit, verbose=True) agent_executor\\\\xa0...\\'}, {\\'url\\': \\'https://wikidocs.net/233351\\', \\'content\\': \\'ë˜í•œ LangChainì€ í¬ë§·í•˜ëŠ” ë™ì•ˆ ë Œë”ë§í•  ë©”ì‹œì§€ë¥¼ ì™„ì „íˆ ì œì–´í•  ìˆ˜ ìˆëŠ” MessagePlaceholder ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë©”ì‹œì§€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì–´ë–¤ ì—­í• ì„ ì‚¬ìš©í•´ì•¼ í• ì§€ í™•ì‹¤\\\\xa0...\\'}, {\\'url\\': \\'https://ncsoft.github.io/ncresearch/f4a00ed849299e3c91fb3244e74ea7f9b974ebb7\\', \\'content\\': \\'Jun 23, 2023 Â· LangChainì€ ì´ëŸ° LLMì„ ì¢€ ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°œë…ë“¤ì„ ì¶”ìƒí™”í•˜ì—¬, LLMì„ ì‚¬ìš©í•˜ë©´ì„œ í¸ë¦¬í• ë§Œí•œ íŒ¨í„´ë“¤ì„ ê·œê²©í™”ì‹œí‚¨ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ê¸€\\\\xa0...\\'}, {\\'url\\': \\'https://13akstjq.github.io/TIL/post/2024-07-12-GenAIwithPythonLLMvsAgents\\', \\'content\\': \\'Jul 12, 2024 Â· ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: LangChain â€” ê±°ì˜ ëª¨ë“  LLM ê¸°ëŠ¥ì„ í¬í•¨í•˜ëŠ” ë§¤ì‰¬ì—… í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Agentê°€ ë‚®ì€ ìˆ˜ì¤€ì—ì„œ ì½”ë”©ë˜ì–´ì•¼ í•˜ëŠ” ê²½\\\\xa0...\\'}, {\\'url\\': \\'https://wikidocs.net/234282\\', \\'content\\': \\'Mar 19, 2024 Â· langchain ì€ ì–¸ì–´ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì´ ì¤‘ ê²€ìƒ‰ ë„êµ¬ ìƒì„± ê¸°ëŠ¥ì€ ë°ì´í„° ê²€ìƒ‰ ë° ì²˜ë¦¬ ì‘ì—…ì„ ìš©ì´í•˜ê²Œ í•œë‹¤.\\'}]\\n```\\n\\n### ì¶”ê°€ëœ TavilySearch ë„êµ¬ (ì»¤ìŠ¤í…€ êµ¬í˜„)\\n\\nTavily ê²€ìƒ‰ APIì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•œ ì„¤ëª…\\n\\n**ê¸°ë³¸ ê²€ìƒ‰ ì„¤ì •**\\n\\n* `query` (str): ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” í‚¤ì›Œë“œë‚˜ ë¬¸ì¥\\n* `search_depth` (str): ê²€ìƒ‰ì˜ ìƒì„¸ë„. \"basic\"(ê¸°ë³¸) ë˜ëŠ” \"advanced\"(ê³ ê¸‰) ì¤‘ ì„ íƒ\\n* `topic` (str): ê²€ìƒ‰ ì£¼ì œ ë¶„ì•¼. \"general\"(ì¼ë°˜) ë˜ëŠ” \"news\"(ë‰´ìŠ¤) ì¤‘ ì„ íƒ\\n* `days` (int): ê²€ìƒ‰ ê²°ê³¼ì˜ ìµœì‹ ì„±. ì§€ì •ëœ ì¼ìˆ˜ ì´ë‚´ì˜ ê²°ê³¼ë§Œ ë°˜í™˜\\n* `max_results` (int): ë°˜í™˜ë°›ì„ ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜\\n\\n**ë„ë©”ì¸ í•„í„°ë§**\\n\\n* `include_domains` (list): ê²€ìƒ‰ ê²°ê³¼ì— ë°˜ë“œì‹œ í¬í•¨í•  ë„ë©”ì¸ ëª©ë¡\\n* `exclude_domains` (list): ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œì™¸í•  ë„ë©”ì¸ ëª©ë¡\\n\\n**ê²°ê³¼ ìƒì„¸ ì„¤ì •**\\n\\n* `include_answer` (bool): APIê°€ ìƒì„±í•œ ë‹µë³€ í¬í•¨ ì—¬ë¶€\\n* `include_raw_content` (bool): ì›¹í˜ì´ì§€ì˜ ì›ë³¸ HTML ì½˜í…ì¸  í¬í•¨ ì—¬ë¶€\\n* `include_images` (bool): ê´€ë ¨ ì´ë¯¸ì§€ ì •ë³´ í¬í•¨ ì—¬ë¶€\\n* `format_output` (bool): ê²€ìƒ‰ ê²°ê³¼ì˜ í¬ë§·íŒ… ì ìš© ì—¬ë¶€\\n\\n**ê¸°íƒ€**\\n\\n* `**kwargs`: ì¶”ê°€ì ì¸ í‚¤ì›Œë“œ ì¸ì. APIì˜ í–¥í›„ ì—…ë°ì´íŠ¸ë‚˜ íŠ¹ìˆ˜ ê¸°ëŠ¥ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ\\n\\n```\\n# !pip install -qU langchain-teddynote \\n```\\n\\n```\\nfrom langchain_teddynote.tools.tavily import TavilySearch # ê¸°ë³¸ ì˜ˆì œ tavily_tool = TavilySearch() # include_domains ì‚¬ìš© ì˜ˆì œ # íŠ¹ì • ë„ë©”ì¸ë§Œ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ tavily_tool_with_domains = TavilySearch(include_domains=[\"github.io\", \"naver.com\"]) # exclude_domains ì‚¬ìš© ì˜ˆì œ # íŠ¹ì • ë„ë©”ì¸ì„ ì œì™¸í•˜ê³  ê²€ìƒ‰ tavily_tool_exclude = TavilySearch(exclude_domains=[\"ads.com\", \"spam.com\"]) # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ì˜ˆì œ result1 = tavily_tool.search( query=\"ìœ íŠœë²„ í…Œë””ë…¸íŠ¸ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜\", # ê²€ìƒ‰ ì¿¼ë¦¬ search_depth=\"advanced\", # ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜ì¤€ topic=\"general\", # ì¼ë°˜ ì£¼ì œ days=7, # ìµœê·¼ 7ì¼ ë‚´ ê²°ê³¼ max_results=10, # ìµœëŒ€ 10ê°œ ê²°ê³¼ include_answer=True, # ë‹µë³€ í¬í•¨ include_raw_content=True, # ì›ë³¸ ì½˜í…ì¸  í¬í•¨ include_images=True, # ì´ë¯¸ì§€ í¬í•¨ format_output=True, # ê²°ê³¼ í¬ë§·íŒ… ) # ë‰´ìŠ¤ ê²€ìƒ‰ ì˜ˆì œ result2 = tavily_tool.search( query=\"ìµœì‹  AI ê¸°ìˆ  ë™í–¥\", # ê²€ìƒ‰ ì¿¼ë¦¬ search_depth=\"basic\", # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜ì¤€ topic=\"news\", # ë‰´ìŠ¤ ì£¼ì œ days=3, # ìµœê·¼ 3ì¼ ë‚´ ê²°ê³¼ max_results=5, # ìµœëŒ€ 5ê°œ ê²°ê³¼ include_answer=False, # ë‹µë³€ ë¯¸í¬í•¨ include_raw_content=False, # ì›ë³¸ ì½˜í…ì¸  ë¯¸í¬í•¨ include_images=False, # ì´ë¯¸ì§€ ë¯¸í¬í•¨ format_output=True, # ê²°ê³¼ í¬ë§·íŒ… ) # íŠ¹ì • ë„ë©”ì¸ í¬í•¨ ê²€ìƒ‰ ì˜ˆì œ result3 = tavily_tool_with_domains.search( query=\"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° íŒ\", # ê²€ìƒ‰ ì¿¼ë¦¬ search_depth=\"advanced\", # ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜ì¤€ max_results=3, # ìµœëŒ€ 3ê°œ ê²°ê³¼ ) # íŠ¹ì • ë„ë©”ì¸ ì œì™¸ ê²€ìƒ‰ ì˜ˆì œ result4 = tavily_tool_exclude.search( query=\"ê±´ê°•í•œ ì‹ë‹¨\", # ê²€ìƒ‰ ì¿¼ë¦¬ search_depth=\"basic\", # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜ì¤€ days=30, # ìµœê·¼ 30ì¼ ë‚´ ê²°ê³¼ max_results=7, # ìµœëŒ€ 7ê°œ ê²°ê³¼ ) # ê²°ê³¼ ì¶œë ¥ print(\"ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼:\", result1) print(\"ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼:\", result2) print(\"íŠ¹ì • ë„ë©”ì¸ í¬í•¨ ê²€ìƒ‰ ê²°ê³¼:\", result3) print(\"íŠ¹ì • ë„ë©”ì¸ ì œì™¸ ê²€ìƒ‰ ê²°ê³¼:\", result4) \\n```\\n\\n### Image ìƒì„± ë„êµ¬ (DALL-E)\\n\\n* `DallEAPIWrapper í´ë˜ìŠ¤`: OpenAIì˜ DALL-E ì´ë¯¸ì§€ ìƒì„±ê¸°ë¥¼ ìœ„í•œ ë˜í¼(wrapper)ì…ë‹ˆë‹¤.\\n\\nì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ DALL-E APIë¥¼ ì‰½ê²Œ í†µí•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì„¤ì • ì˜µì…˜ì„ í†µí•´ ìœ ì—°í•˜ê³  ê°•ë ¥í•œ ì´ë¯¸ì§€ ìƒì„± ë„êµ¬ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**ì£¼ìš” ì†ì„±**\\n\\n* `model`: ì‚¬ìš©í•  DALL-E ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: \"dall-e-2\", \"dall-e-3\")\\n* `n`: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)\\n* `size`: ìƒì„±í•  ì´ë¯¸ì§€ í¬ê¸°\\n* \"dall-e-2\": \"1024x1024\", \"512x512\", \"256x256\"\\n* \"dall-e-3\": \"1024x1024\", \"1792x1024\", \"1024x1792\"\\n* `style`: ìƒì„±ë  ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’: \"natural\", \"vivid\")\\n* `quality`: ìƒì„±ë  ì´ë¯¸ì§€ì˜ í’ˆì§ˆ (ê¸°ë³¸ê°’: \"standard\", \"hd\")\\n* `max_retries`: ìƒì„± ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜\\n\\n**ì£¼ìš” ê¸°ëŠ¥**\\n\\n* DALL-E APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„¤ëª…ì— ê¸°ë°˜í•œ ì´ë¯¸ì§€ ìƒì„±\\n\\n**íë¦„ ì •ë¦¬**\\n\\në‹¤ìŒì€ DALL-E Image Generator ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\nì´ë²ˆì—ëŠ” `DallEAPIWrapper` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤.\\n\\nì´ë•Œ ì…ë ¥ í”„ë¡¬í”„íŠ¸ëŠ” LLM ëª¨ë¸ì—ê²Œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.\\n\\n```\\nfrom langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import PromptTemplate from langchain_openai import ChatOpenAI # ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™” llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9, max_tokens=1000) # DALL-E ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ prompt = PromptTemplate.from_template( \"Generate a detailed IMAGE GENERATION prompt for DALL-E based on the following description. \" \"Return only the prompt, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the prompt\" \"Output should be less than 1000 characters. Write in English only.\" \"Image Description: \\\\n{image_desc}\", ) # í”„ë¡¬í”„íŠ¸, LLM, ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„± chain = prompt | llm | StrOutputParser() # ì²´ì¸ ì‹¤í–‰ image_prompt = chain.invoke( {\"image_desc\": \"ìŠ¤ë§ˆíŠ¸í°ì„ ë°”ë¼ë³´ëŠ” ì‚¬ëŒë“¤ì„ í’ìí•œ neo-classicism painting\"} ) # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì¶œë ¥ print(image_prompt) \\n```\\n\\n```\\nCreate a neo-classical painting that satirically depicts a group of people intently gazing at their smartphones. The scene should be set in a grand, classical architectural setting with marble columns and intricate details reminiscent of ancient Greece. The figures, dressed in elegant, flowing garments typical of neo-classical art, exhibit exaggerated expressions of fascination and distraction as they interact with their devices. Incorporate elements such as classical sculptures or frescoes in the background that contrast with the modernity of the smartphones. Use soft, natural lighting to enhance the scene, and include subtle hints of irony, perhaps by depicting traditional activities or conversations happening just out of focus, symbolizing the disconnect in modern life. \\n```\\n\\nê·¸ëŸ¼, ì´ì „ì— ìƒì„±í•œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ `DallEAPIWrapper` ì— ì…ë ¥í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤.\\n\\n```\\n# DALL-E API ë˜í¼ ê°€ì ¸ì˜¤ê¸° from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper from IPython.display import Image # DALL-E API ë˜í¼ ì´ˆê¸°í™” # model: ì‚¬ìš©í•  DALL-E ëª¨ë¸ ë²„ì „ # size: ìƒì„±í•  ì´ë¯¸ì§€ í¬ê¸° # quality: ì´ë¯¸ì§€ í’ˆì§ˆ # n: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜ dalle = DallEAPIWrapper(model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1) # ì§ˆë¬¸ query = \"ìŠ¤ë§ˆíŠ¸í°ì„ ë°”ë¼ë³´ëŠ” ì‚¬ëŒë“¤ì„ í’ìí•œ neo-classicism painting\" # ì´ë¯¸ì§€ ìƒì„± ë° URL ë°›ê¸° # chain.invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì„¤ëª…ì„ DALL-E í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜ # dalle.run()ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± image_url = dalle.run(chain.invoke({\"image_desc\": query})) # ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. Image(url=image_url, width=500) \\n```\\n\\n## ì‚¬ìš©ì ì •ì˜ ë„êµ¬(Custom Tool)\\n\\nLangChain ì—ì„œ ì œê³µí•˜ëŠ” ë¹ŒíŠ¸ì¸ ë„êµ¬ ì™¸ì—ë„ ì‚¬ìš©ìê°€ ì§ì ‘ ë„êµ¬ë¥¼ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ë¥¼ ìœ„í•´ì„œëŠ” `langchain.tools` ëª¨ë“ˆì—ì„œ ì œê³µí•˜ëŠ” `tool` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\\n\\n### @tool ë°ì½”ë ˆì´í„°\\n\\nì´ ë°ì½”ë ˆì´í„°ëŠ” í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ë„êµ¬ì˜ ë™ì‘ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**ì‚¬ìš© ë°©ë²•**\\n\\n1. í•¨ìˆ˜ ìœ„ì— `@tool` ë°ì½”ë ˆì´í„° ì ìš©\\n2. í•„ìš”ì— ë”°ë¼ ë°ì½”ë ˆì´í„° ë§¤ê°œë³€ìˆ˜ ì„¤ì •\\n\\nì´ ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ì¼ë°˜ Python í•¨ìˆ˜ë¥¼ ê°•ë ¥í•œ ë„êµ¬ë¡œ ì‰½ê²Œ ë³€í™˜í•  ìˆ˜ ìˆìœ¼ë©°, ìë™í™”ëœ ë¬¸ì„œí™”ì™€ ìœ ì—°í•œ ì¸í„°í˜ì´ìŠ¤ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n\\n```\\nfrom langchain.tools import tool # ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. @tool def add_numbers(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b @tool def multiply_numbers(a: int, b: int) -> int: \"\"\"Multiply two numbers\"\"\" return a * b \\n```\\n\\n```\\n# ë„êµ¬ ì‹¤í–‰ add_numbers.invoke({\"a\": 3, \"b\": 4}) \\n```\\n\\n```\\n7\\n```\\n\\n```\\n# ë„êµ¬ ì‹¤í–‰ multiply_numbers.invoke({\"a\": 3, \"b\": 4}) \\n```\\n\\n```\\n12\\n```\\n\\n### êµ¬ê¸€ ë‰´ìŠ¤ê¸°ì‚¬ ê²€ìƒ‰ ë„êµ¬\\n\\n`langchain-teddynote` íŒ¨í‚¤ì§€ì—ì„œ ì œê³µí•˜ëŠ” `GoogleNews` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ê¸€ ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\\n\\n**ì°¸ê³ **\\n\\n* API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸)\\n\\nnews.google.com ì—ì„œ ì œê³µí•˜ëŠ” ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\\n\\n**ì„¤ëª…**\\n\\n* êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\\n* í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\\n\\n* `k` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)\\n\\nì‚¬ìš©í•˜ê¸° ì „ íŒ¨í‚¤ì§€ë¥¼ ì—…ë°ì´íŠ¸ í•´ì£¼ì„¸ìš”.\\n\\n```\\n# !pip install -qU langchain-teddynote \\n```\\n\\n```\\nfrom langchain_teddynote.tools import GoogleNews # ë„êµ¬ ìƒì„± news_tool = GoogleNews() \\n```\\n\\n```\\n# ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰ news_tool.search_latest(k=5) \\n```\\n\\n```\\n[{\\'url\\': \\'https://news.google.com/rss/articles/CBMid0FVX3lxTE82ZXF0N1g0Tmc0SjdGcTN4T0ZJWE9UTXRJdDJxb0VxVUhUb2t5elFjWjlvcnZJMDZFWVpDMVBsdnozUlRKbXJkSGJSTTJITkF5bTBLekJyN0dYV3E1UDhMN0FQSWo0WGsxMW1Hay1GeWZfaHhkTXVZ0gF3QVVfeXFMTkg4WXh0Xy15VEV2YzRKLUljZEwzZzFlajV1bmdmZ2MzcnRvaGJGbmxmM3gtZFp5a2c4a3Q2cmxPUjNNRTFDdmtzN2h4NnNSUHk3ME9zY3NldmRwWlZqd1UzdzVJOHBwbUxQVTJCdnVrMHhpLTYwdW8?oc=5\\', \\'content\\': \"\\'ì¡°ê±´ë¶€ íœ´í•™\\' ìŠ¹ì¸â€¥ë³µê·€ ì•ˆ í•˜ë©´ ì œì Â·ìœ ê¸‰ - MBC ë‰´ìŠ¤\"}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMid0FVX3lxTE5pWEhEY0Jxd08zQjZ0N3ZySEVDSThRN0NLd2YzV3BRdTZGdmpxbXdpMmZmeE12VWc4cmdQWlJYWGpHMnpua044Y3FmRVZMcEsyU0NsajgycjVlNWI4cU5IRkpGT1ZZbS02LWdpbFhLRVREUGdfQXBV0gFmQVVfeXFMTmU0aU5WV25HTldZSS1JZHVmQmdYZk9PcmtWbXFzNGtLYVRQUmVtMmVHcXN3RHlTem5maENOVFNsYXhlQTFHb0NJM3RsSzg1a1FqeWMyRVJ1Mmp4MUFLZTBRbllyY0FB?oc=5\\', \\'content\\': \\'â€˜í•œë™í›ˆ ê³µê²© ì‚¬ì£¼ ì˜í˜¹â€™ ê¹€ëŒ€ë‚¨, SGIì„œìš¸ë³´ì¦ ê°ì‚¬ì§ ì‚¬í‡´ - ë™ì•„ì¼ë³´\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMikAFBVV95cUxQWFRFelJKZTdwSjVDRFloNUpNbF83dVVLOXpYSnZ3Y25BS25jQUFWdFhuYklic0lQT0VwbWw0WEU5MGs2Wm83Y1lQLTBJakRrQTFFTEp0T09tNzY3NVpyVGhyRjdpQ2FNV3pVOVZZaFpwNEl1Q2Rtb1laa2VDY1o0NFRxZXFPOWFyUmF3X0NLa1jSAaQBQVVfeXFMTWJJX040anAtYl9lbzRidURVSDNmMGFlc0t4eHZDa1ZJWk14Z0tzNDd4dzBRTFVyZm5HTFEtMHFSajhkci1PUmlYTTdoUC11RUdNMDhWaW9JakszTlhDWkdmNFc2Vk9BUXVoZEt4dG1qOWVicUs1dXd6cWdtSHh6T2ZpcU9CQXFvMHp3ZElpQ1dZYUloT1NpUDVNMTRGcFNmeDBPVlQ?oc=5\\', \\'content\\': \\'å°¹ \"í•œÂ·í•„ë¦¬í•€ FTA ë°œíš¨ë˜ë©´ ë¬´ì—­Â·íˆ¬ì íšê¸°ì  í™•ëŒ€ë  ê²ƒ\" - ì¡°ì„ ì¼ë³´\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMickFVX3lxTE1Pd3RtbzBFOEpERWlqYk5ISnIzaldRQ21XdHcyaW5zMWpQYmNWR2ZveHVLRExncDhNX2oweTBFZlhrUV90OVVodE9xaVhqVFc0S1BFWUs5VFFILUExTVdpWE10QUlKbnBrSEgtSkN6Skl3UQ?oc=5\\', \\'content\\': \\'ë¬¸ë‹¤í˜œì”¨ í˜ˆì¤‘ì•Œì½”ì˜¬ë†ë„ 0.149%â€¦ê±¸ì–´ì„œ íŒŒì¶œì†Œ ì´ë™ - í•œê²¨ë ˆ\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMickFVX3lxTE9HTU1JajFMeVRPZkViX2EwZkc3R05ySDdWbEJhdjN0QzNnTmo2V0RMbjZQcFI1YTl6dnVHMlpKU0RsODNuRW5TeWYwczBhNjdkNkhiRzc3cWtzQ0wtaHkwU1MwYjNQc2tLX3RjZjdhbEZpQQ?oc=5\\', \\'content\\': \\'[ë‹¨ë…] ê¹€ê±´í¬ ê²°í˜¼ í›„ì—ë„ â€˜ë„ì´ì¹˜ ì£¼ì‹ë§¤ìˆ˜â€™ ì •í™©â€¦í”ë“¤ë¦¬ëŠ” ìœ¤ í•´ëª… - í•œê²¨ë ˆ\\'}]\\n```\\n\\n```\\n# í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ news_tool.search_by_keyword(\"AI íˆ¬ì\", k=5) \\n```\\n\\n```\\n[{\\'url\\': \\'https://news.google.com/rss/articles/CBMiU0FVX3lxTFB1VDRjTlIxeHRjV3NKaERHMFJLOFNpUlNUVnZzQ0JvOUpISVE2cW9rbTZPVU9lQXZuUURJNHF0aDZyaDdQM0F5NFhsc0NrN1dHNEZz?oc=5\\', \\'content\\': \\'ê¸€ë¡œë²Œ CEOë“¤ \"í–¥í›„ 3ë…„ ë¬´ì¡°ê±´ AI íˆ¬ìÂ·Â·Â·ì§ì›ë„ ë” ë½‘ì„ ê²ƒ\" - ë„¤ì´íŠ¸ ë‰´ìŠ¤\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiakFVX3lxTE9BQUtjd3pDS1dXRkZQTlo4Y2w2d2laZFFQRXdsbGlCMFRTcWJYLTNLaksxbFlnYzk3clVnYlFxZF93U2xubXY1VVhBTDQ2SDhLbm9hcDdXQ3prTlJtV0VCYTFINmNRbjR5N3c?oc=5\\', \\'content\\': \\'MIT ê²½ì œí•™ì \"10ë…„ê°„ AIì— ì˜í–¥ë°›ì„ ì§ì—…ì€ 5% ë¶ˆê³¼...ê¸°ì—…ì€ íˆ¬ìë¹„ë§Œ ë‚ ë¦¬ê²Œ ë  ê²ƒ\" - AIíƒ€ì„ìŠ¤\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiW0FVX3lxTE9oenZObVAzOV9hbUliNGhkLTBXZXZrTGpvT0ljcXlpWE5leld2b3RLbUJFTFRtZnU4c0VFaC1oYkVvTTdESEZCYmZUTDJSMFFYdGVXeXN3OVdRTEE?oc=5\\', \\'content\\': \\'MS, ì´íƒˆë¦¬ì•„ì— AI ì¸í”„ë¼ 6ì¡°4ì²œì–µ íˆ¬ì ê²°ì • - ì—°í•©ë‰´ìŠ¤\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiYEFVX3lxTE5XdlFLT29uWXR5SmRpS054Z2J3VldjZlJETHJIeVRPakFUNEdiOHh0N0hOU21BRTlHN1haRTBUYnlKUEdVdXJWUWdTbG9vczFHeDNPRG1hSkc2NzVRaVBLQQ?oc=5\\', \\'content\\': \\'ê¸€ë¡œë²Œ CEO 64% â€œAIì— ìµœìš°ì„  íˆ¬ìâ€ - ì•„ì‹œì•„ê²½ì œ\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiZEFVX3lxTE02VVREUUxSRExwUkxkY0NGZEw0Z0x0N3FHenZvTXN4TEVlRHEzWEF0SGo4N1Y2Zm8xTE5TaGZDZzNubzFrTmxoUVRiNG9WN01wY2hyUGVyZ1lCS05ua1FLWXYwRTk?oc=5\\', \\'content\\': \\'â€œAIì‹œëŒ€ íˆ¬ì í‚¤ì›Œë“œëŠ” â€˜ê³µê¸‰ ë¶€ì¡±â€™...ì´ˆë°˜ 5ë…„ì´ ê¸°íšŒâ€ [í—¤ëŸ´ë“œ ë¨¸ë‹ˆí˜ìŠ¤íƒ€ 2024] - í—¤ëŸ´ë“œë¯¸ë””ì–´\\'}]\\n```\\n\\n```\\nfrom langchain_teddynote.tools import GoogleNews from langchain.tools import tool from typing import List, Dict # í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ ìƒì„± @tool def search_keyword(query: str) -> List[Dict[str, str]]: \"\"\"Look up news by keyword\"\"\" print(query) news_tool = GoogleNews() return news_tool.search_by_keyword(query, k=5) \\n```\\n\\n```\\n# ì‹¤í–‰ ê²°ê³¼ search_keyword.invoke({\"query\": \"AI íˆ¬ì\"}) \\n```\\n\\n```\\nAI íˆ¬ì \\n```\\n\\n```\\n[{\\'url\\': \\'https://news.google.com/rss/articles/CBMiU0FVX3lxTFB1VDRjTlIxeHRjV3NKaERHMFJLOFNpUlNUVnZzQ0JvOUpISVE2cW9rbTZPVU9lQXZuUURJNHF0aDZyaDdQM0F5NFhsc0NrN1dHNEZz?oc=5\\', \\'content\\': \\'ê¸€ë¡œë²Œ CEOë“¤ \"í–¥í›„ 3ë…„ ë¬´ì¡°ê±´ AI íˆ¬ìÂ·Â·Â·ì§ì›ë„ ë” ë½‘ì„ ê²ƒ\" - ë„¤ì´íŠ¸ ë‰´ìŠ¤\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiakFVX3lxTE9BQUtjd3pDS1dXRkZQTlo4Y2w2d2laZFFQRXdsbGlCMFRTcWJYLTNLaksxbFlnYzk3clVnYlFxZF93U2xubXY1VVhBTDQ2SDhLbm9hcDdXQ3prTlJtV0VCYTFINmNRbjR5N3c?oc=5\\', \\'content\\': \\'MIT ê²½ì œí•™ì \"10ë…„ê°„ AIì— ì˜í–¥ë°›ì„ ì§ì—…ì€ 5% ë¶ˆê³¼...ê¸°ì—…ì€ íˆ¬ìë¹„ë§Œ ë‚ ë¦¬ê²Œ ë  ê²ƒ\" - AIíƒ€ì„ìŠ¤\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiW0FVX3lxTE9oenZObVAzOV9hbUliNGhkLTBXZXZrTGpvT0ljcXlpWE5leld2b3RLbUJFTFRtZnU4c0VFaC1oYkVvTTdESEZCYmZUTDJSMFFYdGVXeXN3OVdRTEE?oc=5\\', \\'content\\': \\'MS, ì´íƒˆë¦¬ì•„ì— AI ì¸í”„ë¼ 6ì¡°4ì²œì–µ íˆ¬ì ê²°ì • - ì—°í•©ë‰´ìŠ¤\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiYEFVX3lxTE5XdlFLT29uWXR5SmRpS054Z2J3VldjZlJETHJIeVRPakFUNEdiOHh0N0hOU21BRTlHN1haRTBUYnlKUEdVdXJWUWdTbG9vczFHeDNPRG1hSkc2NzVRaVBLQQ?oc=5\\', \\'content\\': \\'ê¸€ë¡œë²Œ CEO 64% â€œAIì— ìµœìš°ì„  íˆ¬ìâ€ - ì•„ì‹œì•„ê²½ì œ\\'}, {\\'url\\': \\'https://news.google.com/rss/articles/CBMiZEFVX3lxTE02VVREUUxSRExwUkxkY0NGZEw0Z0x0N3FHenZvTXN4TEVlRHEzWEF0SGo4N1Y2Zm8xTE5TaGZDZzNubzFrTmxoUVRiNG9WN01wY2hyUGVyZ1lCS05ua1FLWXYwRTk?oc=5\\', \\'content\\': \\'â€œAIì‹œëŒ€ íˆ¬ì í‚¤ì›Œë“œëŠ” â€˜ê³µê¸‰ ë¶€ì¡±â€™...ì´ˆë°˜ 5ë…„ì´ ê¸°íšŒâ€ [í—¤ëŸ´ë“œ ë¨¸ë‹ˆí˜ìŠ¤íƒ€ 2024] - í—¤ëŸ´ë“œë¯¸ë””ì–´\\'}]\\n```\\n\\në§ˆì§€ë§‰ í¸ì§‘ì¼ì‹œ : 2024ë…„ 10ì›” 22ì¼ 9:53 ì˜¤í›„\\n\\nê´‘ê³ ê°€ ì¶œë ¥ë  ìœ„ì¹˜ì…ë‹ˆë‹¤.\\n\\nê´‘ê³ ê°€ ì¶œë ¥ë  ìœ„ì¹˜ì…ë‹ˆë‹¤.\\n\\n[ëŒ“ê¸€ 0](javascript:show_comments();) [í”¼ë“œë°±](#myModal \"í”¼ë“œë°±ì„ ë‚¨ê²¨ì£¼ì„¸ìš”\")\\n\\n[â€» ëŒ“ê¸€ ì‘ì„±ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.](/loginForm) [(ë˜ëŠ” í”¼ë“œë°±ì„ ì´ìš©í•´ ì£¼ì„¸ìš”.)](#myModal)\\n\\n* **ì´ì „ê¸€** : [CH16 ì—ì´ì „íŠ¸(Agent)](javascript:page(233782))\\n* **ë‹¤ìŒê¸€** : [02. ë„êµ¬ ë°”ì¸ë”©(Binding Tools)](javascript:page(262585))\\n\\n  \\n\\n#### ì±…ê°ˆí”¼\\n\\n### ì´ í˜ì´ì§€ì— ëŒ€í•œ í”¼ë“œë°±ì„ ë‚¨ê²¨ì£¼ì„¸ìš”\\n\\n### ëŒ“ê¸€ì„ ì‹ ê³ í•©ë‹ˆë‹¤.'},\n",
       " {'title': 'Introduction to LangChain - worldbank.github.io',\n",
       "  'url': 'https://worldbank.github.io/LLMs-Practical-Guide/notebooks/tunisia-may-24/3-intro-langchain.html',\n",
       "  'content': 'It is always a good idea to read documentation of a framework. Please head over to LangChain website for details of core functionalities, use cases and features. The screenshot below provides a summary of LangChain ecosytem of features and capabilities. The term Chain in LangChain refers to the core concept of chains in LangChain which is a sequence(s) of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL (we will see this later). [...] LCEL is a key part of the LangChain toolkit. We can use it to connect prompts, models, and retrieval components using a pipe (|) operator rather than task-specific classes. It also lets us create complex workflows that work well in production environments. These chains have built-in support for batch processing, streaming, and asynchronous execution. This makes it easy to integrate with other LangChain tools and utilities like LangSmith and LangServe.\\n\\nA few notes about the chain with LCEL [...] Accessing proprietary LLMs (e.g., OpenAI). LangChain has specific packages for working with OpenAI models. For other providers such as Mistral, you need to check LangChain documentation.',\n",
       "  'score': 0.7327644,\n",
       "  'raw_content': '* [Repository](https://github.com/worldbank/LLMs-Practical-Guide \"Source repository\")\\n* [Suggest edit](https://github.com/worldbank/LLMs-Practical-Guide/edit/main/notebooks/tunisia-may-24/3-intro-langchain.ipynb \"Suggest edit\")\\n* [Open issue](https://github.com/worldbank/LLMs-Practical-Guide/issues/new?title=Issue%20on%20page%20%2Fnotebooks/tunisia-may-24/3-intro-langchain.html&body=Your%20issue%20content%20here. \"Open an issue\")\\n\\n* [.ipynb](../../_sources/notebooks/tunisia-may-24/3-intro-langchain.ipynb \"Download source file\")\\n\\n# Introduction to LangChain\\n\\n# Introduction to LangChain[#](#introduction-to-langchain \"Permalink to this heading\")\\n\\n---\\n\\n**Author:** Dunstan Matekenya\\n\\n**Affiliation:** DECAT, The World Bank Group\\n\\n**Date:** May 30, 2024\\n\\n## What you will learn[#](#what-you-will-learn \"Permalink to this heading\")\\n\\nIn this notebook, you will learn the basics of the LangChain platform as follows.\\n\\n1. **LLM capabilities.** Explore LLM capabilities using LangChain\\n2. **Interacting with LLMs.** Use LangChain functions such as chains, prompt templates and more to connect to LLMs\\n3. **RAG.**. Implementing a simple RAG in Langchain by connecting to external documents\\n4. **LangChain Expression Language (LCEL).**. How to use LCEL instead of functions when interacting with LLMs\\n5. **LangChain Agents.**.\\n\\n## Expected Broad Learning Outcomes[#](#expected-broad-learning-outcomes \"Permalink to this heading\")\\n\\n1. **Connecting to LLMs.** An understanding of how to connect to varios open source and proprietary LLMs using Hugging Face and proprietary specific frameworks such as that for OpenAI and Mistral\\n2. **Different LLMs.**. There are many varieties of LLMs: `chat,instruct,question-answer,sentiment-analysis, instruct` and more. Have basic understanding of differences across these models and when to use which one.\\n3. **The role of memory in Chat models.** Understand the importance of having memory in a chatbot and different strategies for doing it with LangChain.\\n4. **The process of implementing RAG in LangChain**. RAG is one of the most commonly used approach for implementing chats as it enables connection to external custom data. Have a good understanding of the main steps involved in implementing a RAG based system-the steps are the same in LangChain and other frameworks.\\n5. **Understand the role vector databases.** Vector databases are an integral part of working with LLMs. make sure you understand how they fit in the ecosystem and why they are important.\\n\\n# Setup[#](#setup \"Permalink to this heading\")\\n\\n---\\n\\n## Import Packages[#](#import-packages \"Permalink to this heading\")\\n\\nWe will import packages as we go so that you appreciate which class we are using.\\n\\n```\\n import os from pathlib import Path\\n```\\n\\n## Setup API Keys[#](#setup-api-keys \"Permalink to this heading\")\\n\\n```\\n# ==================== # Setup API Keys# ====================# Although its not recommended for security, you can also just # paste your API keys \\n```\\n\\n## Setup input directories[#](#setup-input-directories \"Permalink to this heading\")\\n\\nLets organize where our data is stored so that we can easily access it. Please refer to the slides for recommended folder setup. Copy and paste the full paths to your working folder in the variables below.\\n\\n```\\n # Replace this folder with your working folder DIR_WD = Path(\"/Users/dunstanmatekenya/Google Drive/My Drive/GenAI-Course/Mod2-LLM-Overview/\") # data folder DIR_DATA = DIR_WD. joinpath(\"data\") # We can also set file names for data files we will use to save time FILE_HEP_CHAD = DIR_DATA. joinpath(\"Hepatitis-Chad.pdf\") FILE_MIDDLE_EAST_COVID = DIR_DATA. joinpath(\"MidEast-COVID.pdf\") FILE_DENGUE = DIR_DATA. joinpath(\"Dengue-Global-situation.pdf\")\\n```\\n\\n# 1. Exploring Language Tasks that LLMs can Perform[#](#exploring-language-tasks-that-llms-can-perform \"Permalink to this heading\")\\n\\nIn this section, we will explore what type of NLP tasks LLMs can perfom using the Hugging Face transformer package. In some cases, when we specifiy a specific model, the transformers package will take some time to download the model files. Also, the idea here is to show very simple capabilities. In a real world project, you can train and fine-tune the transformer models on your own dataset. For example, to do a fully fledged sentiment analysis with Hugging Face, take a look at [this tutorial] (<https://huggingface.co/blog/sentiment-analysis-python>).\\n\\n> Note that for almost all of these tasks, you can replace the English text with French text and still get similar results\\n\\n## 1. 1 Text and Document Classification[#](#text-and-document-classification \"Permalink to this heading\")\\n\\nText and document classification are closely related tasks. In **text classification**, we assign predefined categories to individual pieces of text while in **document classification** refers to the process of assigning predefined categories to longer pieces of text, such as entire documents, articles, or reports.\\n\\n* **Examples of text classification tasks**. Sentiment Analysis; Intent Detection;\\n* **Examples of document classification**. Topic categorization,\\n\\n### Sentiment Analysis with the Hugging Face Transformers Library[#](#sentiment-analysis-with-the-hugging-face-transformers-library \"Permalink to this heading\")\\n\\n```\\n # We use transformers ```pipeline library from transformers import pipeline llm = pipeline(\"text-classification\") text = \"I\\'m really enjoying my stay in Tunis\" outputs = llm(text) print(outputs[0][\\'label\\'])\\n```\\n\\n## 1.2 Text Generation[#](#text-generation \"Permalink to this heading\")\\n\\nText generation is a process in natural language processing (NLP) where a machine learning model generates coherent and contextually relevant text based on a given input or prompt. This technology is used in various applications such as chatbots, automated content creation, machine translation, and more.\\n\\nIn real life, the text is not always coherent, based on the model, when we use a default model, the results are not good.\\n\\n```\\n llm = pipeline(\"text-generation\") prompt = \"Malawi is famous for \" outputs = llm(prompt, max_length = 100) print(outputs[0][\\'generated_text\\'])\\n```\\n\\n**EXERCISE-0: Try to specify a different Hugging Face model and see if you get better results**\\n\\n## 1.3 Text Summarization[#](#text-summarization \"Permalink to this heading\")\\n\\nText summarization is a natural language processing (NLP) task that involves creating a concise and coherent summary of a longer text document. The goal is to capture the most important information and main ideas while reducing the length of the original text.\\n\\n```\\n llm = pipeline(\"summarization\", model =\"facebook/bart-large-cnn\") long_text =\"\"\"Walking amid Gion\\'s Machiya wooden houses is a mesmerizing experience. The beautifullypreserved structures exuded an old-world charm that transports visitors back in time, making them feellike they had stepped into a living museum. The glow of lanterns lining the narrow streets add to theenchanting ambiance, making each stroll a memorable journey through Japan\\'s rich cultural history. \"\"\" outputs = llm(long_text, max_length = 60, clean_up_tokenization_spaces = True) print(outputs[0][\\'summary_text\\'])\\n```\\n\\n## 1.4 Question-Answering[#](#question-answering \"Permalink to this heading\")\\n\\nQuestion Answering (QA) is one of the most common tasks or use casef for LLMs. In this task, the model is designed to automatically answer questions posed by humans in natural language. QA systems can be built to answer questions from a variety of sources, such as structured databases, knowledge bases, or unstructured text documents.\\n\\n```\\n llm = pipeline(\"question-answering\") context =\"Walking amid Gion\\'s Machiya wooden houses was a mesmerizing experience.\" question = \"What are Machiya houses made of?\" outputs = llm(question = question, context = context) print(outputs[\\'answer\\'])\\n```\\n\\n## 1.5 Language Translation[#](#language-translation \"Permalink to this heading\")\\n\\n```\\n llm = pipeline(\"translation_en_to_fr\") text =\"This is my first time to visit Tunisia.\" outputs = llm(text, clean_up_tokenization_spaces = True) print(outputs[0][\\'translation_text\\'])\\n```\\n\\n# 2. Introducing LangChain Core Functionalities[#](#introducing-langchain-core-functionalities \"Permalink to this heading\")\\n\\nIt is always a good idea to read documentation of a framework. Please head over to [LangChain website](https://www.langchain.com) for details of core functionalities, use cases and features. The screenshot below provides a summary of LangChain ecosytem of features and capabilities. The term **Chain** in LangChain refers to the core concept of **chains** in LangChain which is a sequence(s) of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL (we will see this later).\\n\\n```\\n fromIPython.display import Image\\n```\\n\\n```\\n Image(filename =\\'../images/LangChain-detailed.png\\', width = 500)\\n```\\n\\n## 2.1 Interacting with Models in LangChain[#](#interacting-with-models-in-langchain \"Permalink to this heading\")\\n\\n* General instruction models - Models which can answer questions but are not quite optmized for chat\\n* Chat models are more optimized for question and answering\\n* Prompting templates and techniques\\n\\n### Trying out Open Vs. Proprietary Model[#](#trying-out-open-vs-proprietary-model \"Permalink to this heading\")\\n\\n* **Accessing open source LLMs on Hugging Face.** In order to access open source LLMs from Hugging Face, you need two main inputs: `Hugging Face token` and the model id or url. Recall that you can explore and grab model details from the Hugging Face platform easily. Once you have that we can use `HuggingFaceEndpoint` or `HuggingFaceHub` to access and use the model.\\n* **Accessing proprietary LLMs (e.g., OpenAI).** LangChain has specific packages for working with OpenAI models. For other providers such as Mistral, you need to check [LangChain documentation](https://python.langchain.com/v0.1/docs/integrations/chat/mistralai/).\\n\\n```\\n fromlangchain_community.llms import HuggingFaceEndpoint, HuggingFaceHub # Lets make this a global variable in case we want to use this model # again MODEL_ID_FALCON =\\'tiiuae/falcon-7b-instruct\\' llm = HuggingFaceHub(repo_id = MODEL_ID_FALCON, huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN) question = \\'Can you still have fun\\' output = llm. invoke(question) print(output)\\n```\\n\\n```\\n from langchain_openai import OpenAI # Note that we will be able to select specific OpenAI models # If you have a paid account llm = OpenAI(openai_api_key = OPENAI_API_KEY) question = \\'Can you still have fun\\' output = llm. invoke(question) print(output)\\n```\\n\\n```\\n 2* 1772\\n```\\n\\n**EXERCISE-1. Find another model on Hugging Face to try**\\n\\n* Go to [Hugging Face](https://huggingface.co/models)\\n* Search for **Text Generation** LLMs. Note that large models can be hard and take long to run.\\n* Get the model Id\\n* Initialize the model, and ask it a question/prompt as we did with Falcon model above\\n\\n### . Prompt templates[#](#prompt-templates \"Permalink to this heading\")\\n\\nPrompt templates are used for creating prompts in a more modular way, so they can be reused and built on. Chains act as the glue in LangChain; bringing the other components together into workflows that pass inputs and outputs between the different components\\n\\n* They are recipes for generating prompts\\n* Flexible and modular\\n* Can contain: instructions, examples, and additional context\\n\\n```\\n fromlangchain.prompts import PromptTemplate, ChatPromptTemplate# A String with instructions, same way we create prompts # in GUI based interface such as chatGPT template =\"You are an artificial intelligence assistant, answer the question. {question} \" prompt = PromptTemplate(template = template, input_variables =[\"question\"]) llm = HuggingFaceHub(repo_id = MODEL_ID_FALCON, token = HUGGINGFACEHUB_API_TOKEN)# Create a Chain using the LLMChain() llm_chain = LLMChain(prompt = prompt, llm = llm) question = \"What is LangChain?\" print(llm_chain. run(question))\\n```\\n\\n```\\n --------------------------------------------------------------------------- ValidationError  Cell In[21], line 8  5 template =\"You are an artificial intelligence assistant, answer the question. {question} \"  6 prompt = PromptTemplate(template = template, input_variables =[\"question\"]) ----> 8 llm = HuggingFaceHub(repo_id = MODEL_ID_FALCON, token = HUGGINGFACEHUB_API_TOKEN)  10# Create a Chain using the LLMChain()  11 llm_chain = LLMChain(prompt = prompt, llm = llm)File ~/anaconda3/lib/python3.10/site-packages/pydantic/main.py:341,pydantic.main.BaseModel.__init__() ValidationError token extra fields not permitted(type = value_error. extra)\\n```\\n\\n### Chat Models[#](#chat-models \"Permalink to this heading\")\\n\\nChat Models are a core component of LangChain. A chat model is a language model that uses chat messages as inputs and returns chat messages as outputs (as opposed to using plain text)\\n\\n```\\n from langchain_openai import ChatOpenAI fromlangchain.prompts import PromptTemplate, ChatPromptTemplate llm = ChatOpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY) prompt_template = ChatPromptTemplate. from_messages([(\"system\",\"You are a helpful assistant who knows alot about Africa.\"),(\"human\",\"Respond to the question: {question} \")]) full_prompt = prompt_template. format_messages(question = \\'What is the best place to visit in Malawi?\\') llm(full_prompt)\\n```\\n\\n## 2.2. Managing chat model memory[#](#managing-chat-model-memory \"Permalink to this heading\")\\n\\n* A key feature of chatbot applications is the ability to have a conversation, where context from the conversation is stored and available for the model to access for later questions or reference.\\n* Memory is important for conversations with chat models; it opens up the possibility of providing follow-up questions, of building and iterating on model responses, and for chatbots to adapt to the userâ€™s preferences and behaviors.\\n* Although LangChain allows us to customize and optimize in-conversation chatbot memory, it is still limited by the modelâ€™s context window.\\n* An **LLMâ€™s context window** is the amount of input text the model can consider at once when generating a response, and the length of this window varies for different models. LangChain has a standard syntax for optimizing model memory.\\n\\nThere are three LangChain classes for implementing chatbot memory as follows.\\n\\n### The `ChatMessageHistory` Class[#](#the-chatmessagehistory-class \"Permalink to this heading\")\\n\\n* The ChatMessageHistory class stores the full history of messages between the user and model. By providing this to the model, we can provide follow-up questions and iterate on the response message.\\n* When additional user messages are provided, the model bases its response on the full context stored in the conversation history\\n* We can use different tools to manage memory usage in LLM applications, and we can even integrate external data to give the models even more context.\\n\\n### The `ConversationBufferMemory` class[#](#the-conversationbuffermemory-class \"Permalink to this heading\")\\n\\n* This gives the application a rolling buffer memory containing the last few messages in the conversation. Users can specify the number of messages to store with the size argument, and the application will discard older messages as newer ones are added.\\n* To integrate the memory type into model, we use a special type of chain for conversations: `ConversationChain`.\\n\\n### The `ConversationSummaryMemory` class[#](#the-conversationsummarymemory-class \"Permalink to this heading\")\\n\\n* Summarizing important points from a conversation can also be a good way of optimizing memory. The ConversationSummaryMemory class summarizes the conversation over time, condensing the information.\\n* This means that the chat model can remember key pieces of context without needing to store and process the entire conversation history\\n\\n### Trying out the ChatMessageHistory class[#](#trying-out-the-chatmessagehistory-class \"Permalink to this heading\")\\n\\n```\\n chat = ChatOpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY) history = ChatMessageHistory() history. add_ai_message(\"Hi! Ask me anything please.\") history. add_user_message(\"Describe a metaphor for learning LangChain in one sentence.\") chat(history. messages)\\n```\\n\\n```\\n # Ask a question based on the previous messages history. add_user_message(\"Summarize the preceding sentence in fewer words\") chat(history. messages)\\n```\\n\\n```\\n # Ask a question based on the previous messages history. add_user_message(\"Summarize the preceding sentence in fewer words\") chat(history. messages)\\n```\\n\\n### Trying out the ConversationBufferMemory[#](#trying-out-the-conversationbuffermemory \"Permalink to this heading\")\\n\\nFor many applications, storing and accessing the entire conversation history isnâ€™t technically feasible. In these cases, the messages must be condensed while retaining as much relevant context as possible. One common way of doing this is with a memory buffer, which stores only the most recent messages based on the parameter `size`.\\n\\n```\\n fromlangchain.memory import ChatMessageHistory, ConversationBufferMemory, ConversationSummaryMemory fromlangchain.chains import LLMChain, ConversationChain, RetrievalQA, RetrievalQAWithSourcesChain # Create an Open AI Chat Model chat = OpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY) # Create the memory object with size set to 2 memory = ConversationBufferMemory(size = 4) buffer_chain = ConversationChain(llm = chat, memory = memory, verbose = True) # buffer_chain. predict(input = \"Describe a language model in one sentence\") buffer_chain. predict(input = \"Describe it again using less words\") buffer_chain. predict(input = \"Describe it again fewer words but at least one word\") buffer_chain. predict(input =\"What did I first ask you? I forgot.\")\\n```\\n\\n```\\n> Entering new ConversationChain chain...The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Describe a language model in one sentenceAI:\\n```\\n\\n```\\n> Finished chain.> Entering new ConversationChain chain...The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Describe a language model in one sentenceAI: A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.Human: Describe it again using less wordsAI:\\n```\\n\\n```\\n> Finished chain.> Entering new ConversationChain chain...The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Describe a language model in one sentenceAI: A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.Human: Describe it again using less wordsAI: A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.Human: Describe it again fewer words but at least one wordAI:\\n```\\n\\n```\\n> Finished chain.> Entering new ConversationChain chain...The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Describe a language model in one sentenceAI: A language model is a statistical model that is trained on a large corpus of text and is able to generate coherent and grammatically correct sentences based on the patterns and structures it has learned.Human: Describe it again using less wordsAI: A language model is a computer program that can generate sentences based on patterns it has learned from a large amount of text.Human: Describe it again fewer words but at least one wordAI: A language model is a program that generates sentences from text patterns.Human: What did I first ask you? I forgot.AI:\\n```\\n\\n```\\n> Finished chain.\\n```\\n\\n**EXERCISE-2. For the `ConversationBufferMemory`, change the buffer size to 1 or 2 and see what happens**\\n\\n## ConversationSummaryMemory[#](#conversationsummarymemory \"Permalink to this heading\")\\n\\nFor longer conversations, storing the entire memory, or even a long buffer memory, may not be technically feasible. In these cases, a summary memory implementation can be a good option. Summary memories summarize the conversation at each step to retain the key context for the model to use. This works by using another LLM for generating the summaries, alongside the LLM used for generating the responses.\\n\\n```\\n# ============================================== # PLEASE FOLLOW INSTRUCTIONS AND COMPLETE CODE# ============================================== # Use openAI model from earlier as a summary model summary_llm = YOUR CODE HERE # Complete code below by putting in summary model above memory = ConversationSummaryMemory(llm = summary_llm)# Create a chat model to use in the Conversation chain below (refer # previous cells where we created OpenAI chat model chat_model = YOUR CODE HERE # Create a conversation chain as we did before summary_chain = YOUR CODE HERE summary_chain. predict(input =\"Please tell me about Malawi.\") summary_chain. predict(input = \"Does that affect Malawi\\'s income?\")\\n```\\n\\n# 3. Adding External Documents to LLMs[#](#adding-external-documents-to-llms \"Permalink to this heading\")\\n\\nAs mentioned in the lectures, LLMs are trained on a specific dataset (often publicly available internet data) up to some point in time. Therefore, if you have some custom organization documents or data, the LLMs will not be able to provide answers based on that information. Furthermore, if there is any new information which came after the LLM was trained, the LLM will not have that information either.\\n\\nThe main remedy to deal with this is to provide the LLM with external documents. Adding external documents further helps with **hallucinations** as the LLM has little opportunity to make up stuff (hallucinate) when it has access to this extra knowledge.\\n\\nIn LangChain, there are three main steps to provide external documents to the LLM (essentially create a Retrieval Augmented Generation)-**RAG Chatbot**\\n\\n1. Identify the data sources (documents, datasets, websites, databases etc).\\n2. Load the documents into LangChain using document loaders. LangChain can work with different document sources, please see [the documentation](https://python.langchain.com/v0.1/docs/integrations/document_loaders/).\\n3. Splitting the documents into chunks.\\n4. Create vector embeddings and store into a vector database for retrievval\\n\\n## 3.1 Document Loaders[#](#document-loaders \"Permalink to this heading\")\\n\\nLangChain has more than 160 document loaders. Some loaders are provided by 3rd parties who manage unique document formats. These include Amazon S3, Microsoft, Google Cloud, Jupyter notebooks, pandas DataFrames, unstructured HTML, YouTube audio transcripts, and more.\\n\\n### PDF Document Loader[#](#pdf-document-loader \"Permalink to this heading\")\\n\\n* Requires installation of the `pypdf` package as a dependency.\\n* There are many different types of PDF loaders in LangChain, and there is documentation available online for each.\\n\\n```\\n!   \\n```\\n\\n```\\n fromlangchain.document_loaders import PyPDFLoader loader = PyPDFLoader(str(FILE_DENGUE)) data = loader. load() print(data[0])\\n```\\n\\n**EXERCISE-3. Explore other LangChain Loaders**\\n\\nCheck the LangChain [document loaders documentation](https://python.langchain.com/v0.1/docs/integrations/document_loaders/) and also check [here](https://python.langchain.com/v0.1/docs/modules/data_connection/) for most commonly used loaders.\\n\\n1. Identify 5 document loaders you find interesting. What are third party document loaders?\\n2. **HTML loaders**. Explore the html or webpage loaders.\\n3. Pick one of your favourite webpages and load it using the `UnstructuredHTMLLoader` loader module. Refer to the [documentation](#UnstructuredHTMLLoader) on how to import the module.\\n4. How do you think this changes your approach to `web-scraping`. Do you think web scraping will change or not with this new capabilities to just connect to a website and query it?\\n\\n## 3.2 Preparing documents for vector database and retrieval[#](#preparing-documents-for-vector-database-and-retrieval \"Permalink to this heading\")\\n\\nIn this stage, there are two sub-steps:\\n\\n* The document is split to enhance efficiency in storage, indexing and ultimately efficient retrieval. Furthermore, chunking also helps with ensuring the document (which act as context) can fit in the context window\\n* An embedding model is used to convert the documents into `vector embeddings`\\n* The vectorized data is stored into a vector database.\\n\\n### Splitting/Chunking Documents[#](#splitting-chunking-documents \"Permalink to this heading\")\\n\\n* Given a PDF document, one naive splitting option would be to separate the document into lines as they appear in the document. This would be simple to implement but could be problematic. Key context required for understanding one line is often found in a different line, and these lines would be processed separately, so we need another strategy which can maintain context across pieces of texts in the document-enter the **overlap concept**. We will compare two document splitting methods from LangChain.\\n\\n> * **CharacterTextSplitter** splits text based on a specified separator, looking at individual characters. This method splits based on the separator first, then evaluates chunk size and chunk overlap.\\n> * **RecursiveCharacterTextSplitter** attempts to split by several separators recursively until the chunks fall within the specified chunk size. There are many other methods that use natural language processing to infer meaning and split appropriately. Optimizing this is an active area of research.\\n\\nThere isnâ€™t one strategy that works for all situations when it comes to splitting documents. Itâ€™s often the case of experimenting with multiple methods, and seeing which one strikes the right balance between retaining sufficient context and managing chunk size.\\n\\n#### CharacterTextSplitter[#](#charactertextsplitter \"Permalink to this heading\")\\n\\n```\\n fromlangchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter quote =\\'One machine can do the work of fifty ordinary humans. \\\\No machine can do the work of one extraordinary human.\\' chunk_size = 24 chunk_overlap = 3 ct_splitter = CharacterTextSplitter(separator =\".\", chunk_overlap = chunk_overlap, chunk_size = chunk_size) docs = ct_splitter. split_text(quote)\\n```\\n\\n```\\n docs\\n```\\n\\n#### RecursiveCharacterTextSplitter[#](#recursivecharactertextsplitter \"Permalink to this heading\")\\n\\n```\\n# Using the same variables: chunk_size and chunk_overlap, instatiate RecursiveCharacterTextSplitter rc_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap) docs = rc_splitter. split_text(quote) print(docs)\\n```\\n\\n### Load data into a vector database[#](#load-data-into-a-vector-database \"Permalink to this heading\")\\n\\nAt this stage, you will be faced with a decision to choose which vector database to use. For our simple demonstration purpose, we will use [chromadb](https://www.trychroma.com), an open source vector database solution. The type of vector database solution you choose can depend on numerous factors such as:\\n\\n* How large are the documents you will be processing\\n* How much money you have to spend on the project\\n* Efficiency/latency requirements for your use case, if you need to provide solution in real-time/fast, you may need a different solution\\n* Accuracy requirements. Sometimes there is a tradeoff between accuracy and latecy.\\n* Integration requirements with existing platforms. In somecases, people use `PostgreSQL` because they are already using it and it has enough add on extensions for vector database capabilities.\\n\\nAnother decision choice is the **embedding model**- the LLM which converts the text/documents into vectors. There are many options on the market and the choice comes down to things such as:\\n\\n* Available budget\\n* Compatibility with the LLM you are using in the generation phase. People do use a different embedding model from the generation model\\n\\n> embedding\\\\_llm = Mistral, chat\\\\_model = ChatOpenAI\\n\\n* Nature of documents, size and alot of other factors\\n\\n```\\n!   \\n```\\n\\n```\\n224.7/224.7 kB827.6 kB/s0:00:000:00:01\\n```\\n\\n```\\n from langchain_openai import OpenAIEmbeddings fromlangchain_community.vectorstores import Chroma fromlangchain_community.embeddings import HuggingFaceEmbeddings # Lets load the Cholera paper and then store it in a database loader = PyPDFLoader(str(FILE_HEP_CHAD)) data = loader. load() chunk_size = 100 chunk_overlap = 10 # Split with RecursiveCharacterTextSplitter rc_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap) docs = rc_splitter. split_documents(data) # Lets use openAI embedding model#embedding_model = OpenAIEmbeddings(openai_api_type=OPENAI_API_KEY) embedding_model = HuggingFaceEmbeddings(model_name =\"all-MiniLM-L6-v2\")# Directory to store our database-set this to the data directory vectordb = Chroma(persist_directory = str(DIR_DATA), embedding_function = embedding_model) # Store the databse vectordb. persist() # Create the database docstorage = Chroma. from_documents(docs, embedding_model)\\n```\\n\\n**EXERCISE-4. Explore what functionality is available under the database object `docstorage_cholera`**\\n\\n* You can use `dir(object)` to check available attributes and functions\\n* Note that there many search related functions which enables you to control how user queries are searcherd when building Chatbots\\n\\n## 3.3 Retrieval[#](#retrieval \"Permalink to this heading\")\\n\\nNow that we have added our external file. Lets use the added document as context in our LLM chains and ask questions again.\\n\\n```\\n fromlangchain.chains import RetrievalQA # Create LLM as before llm = OpenAI(openai_api_key = OPENAI_API_KEY) # Create retriever with qa = RetrievalQA. from_chain_type(llm = llm, chain_type = \"stuff\", retriever = docstorage. as_retriever()) # The question we will ask the LLM # You can ask these questions in French and LLM will also answer in French question = \"Are there any disease outbreaks in Chad?\" # Answer without RAG output = llm. invoke(question) print() print(\"=\"* 60) print(\"LLM Output without using RAG-external document from WHO website\") print(\"=\"* 60) print(output)# For RAG Chain, we put in the question as dictionary print() print(\"=\"* 60) print(\"LLM Output with RAG-external document from WHO website\") print(\"=\"* 60) print(qa. run(question))\\n```\\n\\n**EXERCISE-5. Implement a simple RAG as we did above**\\n\\n1. Use the `FILE_MIDDLE_EAST_COVID` file to create a new Chroma database\\n2. Implement a RAG chainas we did above.\\n3. Compare answers between a the LLM with RAG and no RAG\\n\\n**Hint.** Copy and paste the code from above and edit it.\\n\\n## 3.4 Retrieval with sources reference[#](#retrieval-with-sources-reference \"Permalink to this heading\")\\n\\nIn reallife applications, you will have hundreds or thousands of documents. A user of your system may need to know the spurce of the answrs they are getting. Most RAG systems are able to provide details of where the information is coming from. For example, in the RAG-Malawi example, the RAG system can provide the page numbers. In this case, with LangChain, you can you can just provide information about the document where the answer came from.\\n\\nOne method of mitigating the risk of LLM hallucinations from RAG is using RetrievalQAWithSourcesChain, which also returns the data source of the answer. Aside from the chain class, the code is exactly the same as RetrievalQA. However, this class returns a dictionary containing a â€˜sourcesâ€™ key and an â€˜answerâ€™ key. The â€˜sourcesâ€™ key refers to the file where the answer came from, which is helpful when there are many documents in the database.\\n\\n```\\n fromlangchain.chains import RetrievalQAWithSourcesChain qa = RetrievalQAWithSourcesChain. from_chain_type(llm = llm, chain_type = \"stuff\", retriever = docstorage. as_retriever()) results = qa({\"question\": \"Are there any disease outbreaks in Chad?\"}, return_only_outputs = True) print(results)\\n```\\n\\n# 4. LangChain Expression Language (LCEL)[#](#langchain-expression-language-lcel \"Permalink to this heading\")\\n\\n> In summary, LCEL is a different (recommended) syntax of achieving the same things we have done in LangChain\\n\\nLCEL is a key part of the LangChain toolkit. We can use it to connect prompts, models, and retrieval components using a **pipe (|)** operator rather than task-specific classes. It also lets us create complex workflows that work well in production environments. These chains have built-in support for batch processing, streaming, and asynchronous execution. This makes it easy to integrate with other LangChain tools and utilities like **LangSmith** and **LangServe**.\\n\\nA few notes about the chain with LCEL\\n\\n* The `|(pipe)` in LCEL indicates that the output from one component will be used as the input to the next.\\n\\n## 4.1 A Simple Chain with LCEL[#](#a-simple-chain-with-lcel \"Permalink to this heading\")\\n\\n```\\n model = ChatOpenAI(openai_api_key = OPENAI_API_KEY) prompt = ChatPromptTemplate. from_template(\"You are a helpful personal assistant. \\\\Answer the following question: {question} \") # Create Chain in LCEL fashion llm_chain = prompt | model # Recall how we created a chain before #llm_chain = LLMChain(prompt=prompt, llm=llm) # Run using invoke print(llm_chain. invoke(\"What is the capital of Tunisia?\"))\\n```\\n\\n## 4.2 RAG with LCEL[#](#rag-with-lcel \"Permalink to this heading\")\\n\\n```\\n model = ChatOpenAI(openai_api_key = OPENAI_API_KEY) embedding_model = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY) vectorstore = Chroma. from_texts([\"Dunstan stayed in Tunis, the capital of Tunisia from Sunday May 26 to Satarday May 31.\"], embedding = embedding_model) retriever = vectorstore. as_retriever() template =\"\"\"Answer the question based on the context:{context}. Question: {question} \"\"\" prompt = ChatPromptTemplate. from_template(template) chain =({\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()) chain. invoke(\"When did Dunstan visit Tunisia?\")\\n```\\n\\n## 4.3 More things you can do with LCEL[#](#more-things-you-can-do-with-lcel \"Permalink to this heading\")\\n\\nThere are alot of things you can do with LCEL. For example,\\n\\n* **Batch or Streaming**. LCEL chains can be run in `batch` mode or `streaming` mode\\n* **Sequential chains.**. Sequential chains utilize step-by-step processing of inputs, where the output from one step becomes the input for the next. This enables a clear and organized flow of information within the chain. They provide flexibility in constructing custom pipelines by combining different components, such as prompts, models, retrievers, and output parsers, to suit specific use cases and requirements.\\n* **Passing Data Across Chains.** There are many cases where your application will require the use of several chains that pass outputs between them\\n\\n### Using sequential chaining to create Python code and check it with LCEL[#](#using-sequential-chaining-to-create-python-code-and-check-it-with-lcel \"Permalink to this heading\")\\n\\n```\\n coding_prompt = PromptTemplate. from_template( \"\"\"Write Python code to loop through the following list, printing each element: {list}\"\"\") validate_prompt = PromptTemplate. from_template( \"\"\"Consider the following Python code: {answer} If it doesn\\'t use a list comprehension, update it to use one. If it does use a list comprehension, return the original code without explanation:\"\"\") llm = ChatOpenAI(openai_api_key = OPENAI_API_KEY) # Create the sequential chain chain =({\"answer\": coding_prompt | llm | StrOutputParser()} | validate_prompt | llm | StrOutputParser()) # Invoke the chain with the user\\'s question print(chain. invoke({\"list\":\"[3, 1, 4, 1]\"}))\\n```\\n\\n# 5. LangChain Agents[#](#langchain-agents \"Permalink to this heading\")\\n\\nIn LLMs and Gen AI, the idea behind agents is to use language models to determine which a sequence of actions to take to meet a pre-defined objective. Thus, the LLM is able solve complex problems or perform complex tasks by planning, determing what tools to use and what knowledge to get until the task is solved without explicit supervision.\\n\\n* Agents often use tools, which, in LangChain, are functions used by the agent to interact with the system. These tools can be high-level utilities to transform inputs, or they can be specific to a series of tasks. Agents can even use chains and other agents as tools!\\n* In LangChain, there different agent types. See [this documentation](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/) for explanation of how the agents are categorized.\\n\\n## Components of a LangChain Agent[#](#components-of-a-langchain-agent \"Permalink to this heading\")\\n\\nThere are four primary components to LangChain agents.\\n\\n* The user input in the form of a prompt represents the initial input provided by the user.\\n* The definition for handling the intermediate steps explains how to handle and process actions during the agentâ€™s execution.\\n* The agent also needs to have a definition for the tools and model behavior to execute.\\n* The output parser formats the output generated by the model into the most appropriate format for the use case. Agents can be defined for specificity or high-level thought processes.\\n\\n## 5.1 Zero-Shot ReAct agent[#](#zero-shot-react-agent \"Permalink to this heading\")\\n\\nReAct stands for **Reasoning and Acting**. This simplifies the answer to infer as much context as possible. We start by importing the initialize\\\\_agent function and AgentType for agent creation and configuration\\n\\n```\\n fromlangchain.agents import initialize_agent, AgentType, load_tools # Define LLM llm = OpenAI(model_name =\"gpt-3.5-turbo-instruct\", temperature = 0, openai_api_key = OPENAI_API_KEY)# Define what tools the agent will will use, it can be more than one tool tools = load_tools([\"llm-math\"], llm = llm) agent = initialize_agent(tools, llm, agent = AgentType. ZERO_SHOT_REACT_DESCRIPTION, verbose = True) agent. run(\"What is 10 multiplied by 50?\")\\n```\\n\\n## 5.2 Other Agents[#](#other-agents \"Permalink to this heading\")\\n\\nThere are alot of other agents and tools in LangChain. For example, in order to interact with a database or structured dataset we will utilise an `SQLAgent`\\n\\n# 6. Evaluating LLM Outputs in LangChain[#](#evaluating-llm-outputs-in-langchain \"Permalink to this heading\")\\n\\nAs mentioned in Lectures, its important to evaluate LLM model outputs as well as all ML based outputs fot that matter.  \\n Although Gen AI may seem very smart, the models still make alot of mistakes. As such, evaluating AI applications is important for several reasons.\\n\\n* First, it checks if the AI model can accurately interpret and respond to a variety of inputs. This is vital in applications where responses inform decision-making, and reliability is paramount.\\n* Evaluation also help identify the strengths and weaknesses of a model, which allows for targeted and continuous improvements, and builds trust among users and stakeholders.\\n* Evaluation allows us to re-align model output with human intent, getting to the ideal responses faster.\\n\\n## LangChain evaluation tools[#](#langchain-evaluation-tools \"Permalink to this heading\")\\n\\nLangChain has built-in evaluation tools for comparing model outputs based on common criteria, such as relevance and correctness. It also provides tools for defining custom criteria, which we can tailor to specific use cases. Finally, the `QAEvalChain class` is another tool that can be used to measure how well an AIâ€™s response answers a specific question using ground truth responses.\\n\\n## 6.1 LangChain Built-in Evaluation Metrics[#](#langchain-built-in-evaluation-metrics \"Permalink to this heading\")\\n\\n**EXERCISE-6: Explore Evalution Metrics in LangChain**\\n\\n* run this import statement: `fromlangchain.evaluation import Criteria`\\n* use `list` function pn Criteria to check the list of available functions\\n\\n```\\n fromlangchain.chat_models import ChatOpenAI fromlangchain.evaluation import load_evaluator llm = OpenAI(openai_api_key = OPENAI_API_KEY) evaluator = load_evaluator(\"criteria\", criteria = \"relevance\", llm = llm) eval_result = evaluator. evaluate_strings(prediction = \"The capital of New York state is Albany\", input =\"What is 26 + 43?\") print(eval_result)\\n```\\n\\n**EXERCISE-7: Try doing the same evaluation above with a different LLM (e.g., Mistral)**\\n\\n## 6.2 Defining Custom Metrics[#](#defining-custom-metrics \"Permalink to this heading\")\\n\\nTo customize the criteria, we need to evaluate the specific use case and define a dictionary named custom\\\\_criteria. This example adds simplicity, bias, clarity, and truthfulness criteria. Custom criteria work by mapping criteria names to the questions that are used to evaluate the strings. To use these new criteria, create an evaluator object, but this time, using our custom\\\\_critera.\\n\\n```\\n custom_criteria ={\"simplicity\": \"Does the language use brevity?\", \"bias\": \"Does the language stay free of human bias?\", \"clarity\": \"Is the writing easy to understand?\", \"truthfulness\": \"Is the writing honest and factual?\"} evaluator = load_evaluator(\"criteria\", criteria = custom_criteria, llm = llm) eval_result = evaluator. evaluate_strings(input = \"What is the best Italian restaurant in New York City?\", prediction =\"That is a subjective statement and I cannot answer that.\") print(eval_result)\\n```\\n\\n## 6.3 QAEvalChain[#](#qaevalchain \"Permalink to this heading\")\\n\\nQuestion-Answering (QA) is one of the most popular applications LLMs. But it is often not always obvious to determine what parameters (e.g., chunk size) or components (e.g., model choice, VectorDB) yield the best QA performance in the system we are building. The QA eval chain is an LLM chain for evaluting performance of an LLM on QA task. Refer to this detailed [LangChain blog post](https://blog.langchain.dev/auto-eval-of-question-answering-tasks/) for details about QAEvalChain.\\n\\n### 6.3.1 Trying out QAEvalChain[#](#trying-out-qaevalchain \"Permalink to this heading\")\\n\\nAs a metric, QAEvalChain focuses on the **accuracy** and **relevance** of the response. In this chain, RAG will be used to store the document and ground truth responses, and an evaluation model instance is used to compare the semantic meaning of a modelâ€™s results with the ground truth.\\n\\nFirst, we load our data source, in this case, a PDF document, and split it into chunks. Next, we set up the embeddings model, vector database, and LLM, and combine them in a chain. The input\\\\_key is set to â€œquestionâ€, as questions will be used to query the database\\n\\n### Create a RAG Retriever[#](#create-a-rag-retriever \"Permalink to this heading\")\\n\\n```\\n # Lets load the Cholera paper and then store it in a database loader = PyPDFLoader(str(FILE_DENGUE)) data = loader. load() chunk_size = 100 chunk_overlap = 50 # Split with RecursiveCharacterTextSplitter rc_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap) docs = rc_splitter. split_documents(data) # Lets use openAI embedding model embedding_model = OpenAIEmbeddings(openai_api_type = OPENAI_API_KEY)# Directory to store our database-set this to the data directory vectordb = Chroma(persist_directory = str(DIR_DATA), embedding_function = embedding_model) # Store the databse vectordb. persist() # Create the database docstorage = Chroma. from_documents(docs, embedding_model) # LLM llm = OpenAI(model_name =\"gpt-3.5-turbo-instruct\", openai_api_key = OPENAI_API_KEY) # Define the retriever chain qa = RetrievalQA. from_chain_type(llm = llm, chain_type = \"stuff\", retriever = docstorage. as_retriever(), input_key = \"question\")\\n```\\n\\n## Define a Question Set as Key-Value Pairs in a Dict[#](#define-a-question-set-as-key-value-pairs-in-a-dict \"Permalink to this heading\")\\n\\nThis is a ground-truth dataset which a list of questions and their correct responses.\\n\\n```\\n question_set =[{\"question\": \"Did dengue cases increase in 2023?\", \"answer\":\"Yes, in 2023, there was an increase in cases globally.\"},{\"question\":\"According to the document, which are the top four regions affected by arboviral diseases?\", \"answer\": \"Africa is oe of the top four regions\"},{\"question\": \"How is dengue virus transimitted to humans?\", \"answer\": \"through the bite of infected mosquitoes\"}]\\n```\\n\\n## Run QAEVAL[#](#run-qaeval \"Permalink to this heading\")\\n\\n```\\n fromlangchain.evaluation import QAEvalChain predictions = qa. apply(question_set) eval_chain = QAEvalChain. from_llm(llm) results = eval_chain. evaluate(question_set, predictions, question_key = \"question\", prediction_key = \"result\", answer_key = \\'answer\\') print(results)\\n```\\n\\n**EXERCISE-7 (Do this in Your Groups): Run Evaluation on a Custom Eval Dataset for a RAG Chatbot QA Task**\\n\\n1. Create a RAG LLM Chain as we have done before. Please identify a PDF document to use which contains some new information that the LLMs do not have. Note that it can be a French or English document.\\n2. Create 5 pairs of questions and correct answers to use to evaluate your RAG\\n3. Run QAEVAL on the eval dataset and report how many responses did the LLM get correct.\\n4. Do this again with a different LLM (e.g., Falcon or Mistral) and compare performance across models. *Note that your eval dataset remains the same.*\\n\\n# 7. Summary[#](#summary \"Permalink to this heading\")\\n\\n---\\n\\nIn this notebook, we covered the basics of how to use LangChain to interact with both proprietary models from OpenAI and open source LLMs through Hugging Face library. We noted that there are two approaches to building Chains with LangChain: either using the functions or using the LCEL syntax. We covered key topics as follows: creating chains and interacting with LLMs; managing memeory of chat models; setup a RAG based chains which incorprates external documents and evaluating LLM outputs.\\n\\nWhat we have covered in this notebook is the tip of the ice-berg just to get you started on building LLM based applications with LangChain and other tools. There are alot of other things to learn and check.\\n\\n* What are other frameworks whoch perform the same tasks as LangChain?\\n* LangChain Agents and LLM agents in general\\n* Vector databases and their role\\n* How to work with different document sources (e.g., websites)\\n* How to choose embedding models and the influence they have on generation\\n* Which model to use: instruct/chat/text generation\\n* and more\\n\\nContents\\n\\n '},\n",
       " {'title': 'Tools - Docs by LangChain',\n",
       "  'url': 'https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling-errors?ref=blog.langchain.dev',\n",
       "  'content': 'Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema. [...] Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systemsâ€”such as APIs, databases, or file systemsâ€”using structured input.\\nTools are components that agents call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs. [...] Skip to main content\\n\\n Overview\\n\\n Changelog\\n\\n##### Get started\\n\\n Install\\n Quickstart\\n Philosophy\\n\\n##### Core components\\n\\n Agents\\n Models\\n Messages\\n Tools\\n Short-term memory\\n Streaming\\n Structured output\\n\\n##### Middleware\\n\\n Overview\\n Built-in middleware\\n Custom middleware\\n\\n##### Advanced usage\\n\\n Guardrails\\n Runtime\\n Context engineering\\n Model Context Protocol (MCP)\\n Human-in-the-loop\\n Multi-agent\\n Retrieval\\n Long-term memory\\n\\n##### Agent development\\n\\n LangSmith Studio\\n Test\\n Agent Chat UI',\n",
       "  'score': 0.71088976,\n",
       "  'raw_content': '[Skip to main content](#content-area)\\n\\n* [Overview](/oss/javascript/langchain/overview)\\n\\n* [Changelog](/oss/javascript/releases/changelog)\\n\\n##### Get started\\n\\n* [Install](/oss/javascript/langchain/install)\\n* [Quickstart](/oss/javascript/langchain/quickstart)\\n* [Philosophy](/oss/javascript/langchain/philosophy)\\n\\n##### Core components\\n\\n* [Agents](/oss/javascript/langchain/agents)\\n* [Models](/oss/javascript/langchain/models)\\n* [Messages](/oss/javascript/langchain/messages)\\n* [Tools](/oss/javascript/langchain/tools)\\n* [Short-term memory](/oss/javascript/langchain/short-term-memory)\\n* [Streaming](/oss/javascript/langchain/streaming)\\n* [Structured output](/oss/javascript/langchain/structured-output)\\n\\n##### Middleware\\n\\n* [Overview](/oss/javascript/langchain/middleware/overview)\\n* [Built-in middleware](/oss/javascript/langchain/middleware/built-in)\\n* [Custom middleware](/oss/javascript/langchain/middleware/custom)\\n\\n##### Advanced usage\\n\\n* [Guardrails](/oss/javascript/langchain/guardrails)\\n* [Runtime](/oss/javascript/langchain/runtime)\\n* [Context engineering](/oss/javascript/langchain/context-engineering)\\n* [Model Context Protocol (MCP)](/oss/javascript/langchain/mcp)\\n* [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop)\\n* [Multi-agent](/oss/javascript/langchain/multi-agent)\\n* [Retrieval](/oss/javascript/langchain/retrieval)\\n* [Long-term memory](/oss/javascript/langchain/long-term-memory)\\n\\n##### Agent development\\n\\n* [LangSmith Studio](/oss/javascript/langchain/studio)\\n* [Test](/oss/javascript/langchain/test)\\n* [Agent Chat UI](/oss/javascript/langchain/ui)\\n\\n##### Deploy with LangSmith\\n\\n* [Deployment](/oss/javascript/langchain/deploy)\\n* [Observability](/oss/javascript/langchain/observability)\\n\\n* [Create tools](#create-tools)\\n* [Basic tool definition](#basic-tool-definition)\\n* [Reserved argument names](#reserved-argument-names)\\n* [Accessing Context](#accessing-context)\\n* [Context](#context)\\n* [Memory (Store)](#memory-store)\\n* [Stream Writer](#stream-writer)\\n\\n[Core components](/oss/javascript/langchain/agents)\\n\\n# Tools\\n\\nMany AI applications interact with users via natural language. However, some use cases require models to interface directly with external systemsâ€”such as APIs, databases, or file systemsâ€”using structured input.\\nTools are components that [agents](/oss/javascript/langchain/agents) call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs.\\nTools encapsulate a callable function and its input schema. These can be passed to compatible [chat models](/oss/javascript/langchain/models), allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.\\n\\n**Server-side tool use**Some chat models (e.g., [OpenAI](/oss/javascript/integrations/chat/openai), [Anthropic](/oss/javascript/integrations/chat/anthropic), and [Gemini](/oss/javascript/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/javascript/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/javascript/integrations/providers/overview) to learn how to access these tools with your specific chat model.\\n\\n## [\\u200b](#create-tools) Create tools\\n\\n### [\\u200b](#basic-tool-definition) Basic tool definition\\n\\nThe simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use [zod](https://zod.dev/) to define the toolâ€™s input schema:\\n\\nCopy\\n\\n```\\nimport * as z from \"zod\"\\nimport { tool } from \"langchain\"\\n\\nconst searchDatabase = tool(\\n  ({ query, limit }) => `Found ${limit} results for \\'${query}\\'`,\\n  {\\n    name: \"search_database\",\\n    description: \"Search the customer database for records matching the query.\",\\n    schema: z.object({\\n      query: z.string().describe(\"Search terms to look for\"),\\n      limit: z.number().describe(\"Maximum number of results to return\"),\\n    }),\\n  }\\n);\\n\\n```\\n\\npython\\n\\n### [\\u200b](#reserved-argument-names) Reserved argument names\\n\\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\\n\\n| Parameter name | Purpose |\\n| --- | --- |\\n| `config` | Reserved for passing `RunnableConfig` to tools internally |\\n| `runtime` | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\\n\\nTo access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`.\\n:::\\n\\n## [\\u200b](#accessing-context) Accessing Context\\n\\n**Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.The runtime context provides a structured way to supply runtime data, such as DB connections, user IDs, or config, into your tools. This avoids global state and keeps tools testable and reusable.\\n\\n#### [\\u200b](#context) Context\\n\\nTools can access an agentâ€™s runtime context through the `config` parameter:\\n\\nCopy\\n\\n```\\nimport * as z from \"zod\"\\nimport { ChatOpenAI } from \"@langchain/openai\"\\nimport { createAgent } from \"langchain\"\\n\\nconst getUserName = tool(\\n  (_, config) => {\\n    return config.context.user_name\\n  },\\n  {\\n    name: \"get_user_name\",\\n    description: \"Get the user\\'s name.\",\\n    schema: z.object({}),\\n  }\\n);\\n\\nconst contextSchema = z.object({\\n  user_name: z.string(),\\n});\\n\\nconst agent = createAgent({\\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\\n  tools: [getUserName],\\n  contextSchema,\\n});\\n\\nconst result = await agent.invoke(\\n  {\\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\\n  },\\n  {\\n    context: { user_name: \"John Smith\" }\\n  }\\n);\\n\\n```\\n\\n#### [\\u200b](#memory-store) Memory (Store)\\n\\nAccess persistent data across conversations using the store. The store is accessed via `config.store` and allows you to save and retrieve user-specific or application-specific data.\\n\\nCopy\\n\\n```\\nimport * as z from \"zod\";\\nimport { createAgent, tool } from \"langchain\";\\nimport { InMemoryStore } from \"@langchain/langgraph\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\n\\nconst store = new InMemoryStore();\\n\\n// Access memory\\nconst getUserInfo = tool(\\n  async ({ user_id }) => {\\n    const value = await store.get([\"users\"], user_id);\\n    console.log(\"get_user_info\", user_id, value);\\n    return value;\\n  },\\n  {\\n    name: \"get_user_info\",\\n    description: \"Look up user info.\",\\n    schema: z.object({\\n      user_id: z.string(),\\n    }),\\n  }\\n);\\n\\n// Update memory\\nconst saveUserInfo = tool(\\n  async ({ user_id, name, age, email }) => {\\n    console.log(\"save_user_info\", user_id, name, age, email);\\n    await store.put([\"users\"], user_id, { name, age, email });\\n    return \"Successfully saved user info.\";\\n  },\\n  {\\n    name: \"save_user_info\",\\n    description: \"Save user info.\",\\n    schema: z.object({\\n      user_id: z.string(),\\n      name: z.string(),\\n      age: z.number(),\\n      email: z.string(),\\n    }),\\n  }\\n);\\n\\nconst agent = createAgent({\\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\\n  tools: [getUserInfo, saveUserInfo],\\n  store,\\n});\\n\\n// First session: save user info\\nawait agent.invoke({\\n  messages: [\\n    {\\n      role: \"user\",\\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\\n    },\\n  ],\\n});\\n\\n// Second session: get user info\\nconst result = await agent.invoke({\\n  messages: [\\n    { role: \"user\", content: \"Get user info for user with id \\'abc123\\'\" },\\n  ],\\n});\\n\\nconsole.log(result);\\n// Here is the user info for user with ID \"abc123\":\\n// - Name: Foo\\n// - Age: 25\\n// - Email: foo@langchain.dev\\n\\n```\\n\\n#### [\\u200b](#stream-writer) Stream Writer\\n\\nStream custom updates from tools as they execute using `config.streamWriter`. This is useful for providing real-time feedback to users about what a tool is doing.\\n\\nCopy\\n\\n```\\nimport * as z from \"zod\";\\nimport { tool } from \"langchain\";\\n\\nconst getWeather = tool(\\n  ({ city }, config) => {\\n    const writer = config.streamWriter;\\n\\n    // Stream custom updates as the tool executes\\n    writer(`Looking up data for city: ${city}`);\\n    writer(`Acquired data for city: ${city}`);\\n\\n    return `It\\'s always sunny in ${city}!`;\\n  },\\n  {\\n    name: \"get_weather\",\\n    description: \"Get weather for a given city.\",\\n    schema: z.object({\\n      city: z.string(),\\n    }),\\n  }\\n);\\n\\n```\\n\\n---\\n\\n[Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)\\n\\n[Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\\n\\nWas this page helpful?\\n\\n[Messages\\n\\nPrevious](/oss/javascript/langchain/messages)\\n[Short-term memory](/oss/javascript/langchain/short-term-memory)'},\n",
       " {'title': 'LangChainì— ëŒ€í•˜ì—¬ |',\n",
       "  'url': 'https://ncsoft.github.io/ncresearch/f4a00ed849299e3c91fb3244e74ea7f9b974ebb7',\n",
       "  'content': '1. ì±—ë´‡ì˜ ì •ì±…ì„ ì„¸ìš°ê¸° ìœ„í•´ ìˆ¨ê²¨ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì •í•˜ê³  ë§¤ í˜¸ì¶œë§ˆë‹¤ LLMì´ í•´ë‹¹ ë‚´ìš©ì„ ìµœì´ˆì— ì œê³µë°›ì„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\\n2. ëŒ€í™”ì˜ ë§¥ë½ì„ ê¸°ì–µí•©ë‹ˆë‹¤.\\n3. ëŒ€í™”ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì• ë¶€ë¶„ì„ ìë¥´ê±°ë‚˜ ìš”ì•½í•˜ëŠ” ë“± LLMì´ ì²˜ë¦¬í•˜ê¸° ìš©ì´í•œ ì‚¬ì´ì¦ˆë¥¼ ìœ ì§€í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\\n4. LLMì˜ ë‹µë³€ì—ì„œ ì¸í„°ë„·ì— ì ‘ì†í•˜ì—¬ ë‚´ìš©ì„ ë³´ì¶©í•  ìˆ˜ ìˆì„ë§Œí•œ ë¶€ë¶„ì„ ì¸í„°ë„·ì— ì ‘ì†í•˜ì—¬ ë³´ì¶©í•©ë‹ˆë‹¤. (Bingì˜ BingChatì´ë‚˜ Googleì˜ Bardì˜ ê¸°ëŠ¥)\\n\\nì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ LangChainì€ ì—¬ëŸ¬ í˜•íƒœì˜ ì¶”ìƒí™”ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ê³ , ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µë˜ëŠ” êµ¬í˜„ì„ ì‚¬ìš©í•˜ê±°ë‚˜ í˜¹ì€ ìê¸°ê°€ ì›í•˜ëŠ” ì •ì±…ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ì—°ì‡„ì ì¸ ì²´ì¸(Chain)ì˜ í•œ ë¶€ë¶„ìœ¼ë¡œ ë™ì‘í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ LangChainì—ëŠ” ì±—ë´‡ ë¿ë§Œì´ ì•„ë‹ˆë¼, ì—°ì‡„ì ì¸ ë™ì‘ì´ í•„ìš”í•œ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë„ í•´ê²°í•˜ë ¤ê³  í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë°ì— ì£¼ì¶•ì´ ë˜ëŠ” ì¤‘ìš”í•œ ëª¨ë“ˆë“¤ì— ëŒ€í•´ ìš°ì„  ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. [...] # LangChainì— ëŒ€í•˜ì—¬\\n\\nì˜¤ì§„ê· (Oh Jinkyun)\\n\\nJune 23, 2023\\n\\n ê°œìš”\\n í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œ\\n ëª¨ë“ˆ\\n ì˜ˆì œ: ë‹¨ìˆœí•œ ì±—ë´‡\\n  + ë‹¨ìˆœ ì§ˆë¬¸-ì‘ë‹µê¸°\\n  + ëŒ€í™” ë§¥ë½ ê¸°ì–µí•˜ê¸°\\n  + ê¸¸ì´ ì¤„ì´ê¸°\\n ì˜ˆì œ: Agent\\n ì˜ˆì œ: ì‹¤ì œ LLM(OpenAI)ìœ¼ë¡œ Agent ì‹¤í–‰\\n ê²°ë¡ \\n References\\n\\n# ê°œìš”\\n\\nOpenAIì˜ ChatGPT ì´ë˜ë¡œ LLM(Large Language Model)ì€ AI, NLP ê´€ë ¨ìë“¤ ë¿ë§Œ ì•„ë‹ˆë¼ ì‚¬íšŒ ì „ë°˜ì ì¸ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nLangChainì€ ì´ëŸ° LLMì„ ì¢€ ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°œë…ë“¤ì„ ì¶”ìƒí™”í•˜ì—¬, LLMì„ ì‚¬ìš©í•˜ë©´ì„œ í¸ë¦¬í• ë§Œí•œ íŒ¨í„´ë“¤ì„ ê·œê²©í™”ì‹œí‚¨ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\\n\\nì´ ê¸€ì—ì„œëŠ” LangChainì˜ ì£¼ìš” ëª¨ë“ˆì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ê³¼, LangChainì´ ì–´ë–¤ í¸ë¦¬í•¨ì„ ì œê³µí•˜ê³  ì–´ë–»ê²Œ í™œìš©í•´ë³¼ ìˆ˜ ìˆì„ì§€ ì˜ˆì œë¥¼ í†µí•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. [...] LLMì´ ì§ì ‘ ë‹µí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì™¸ë¶€ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•˜ê±°ë‚˜ ìœ ìš©í•œ ê²ƒë“¤, ì´ë¥¼í…Œë©´ ìˆ˜í•™ì ì¸ ê³„ì‚°ì´ë‚˜ ì˜ êµ¬ì¡°í™” ëœ ë°ì´í„°ì˜ ì¡°íšŒ(ì˜ˆì‹œ: íŠ¹ì • ë°°ìš°ì˜ ë‚˜ì´, ì¶œì—°ì‘ì„ ì¡°íšŒí•˜ê³  ì‹¶ë‹¤)ê°™ì€ LLM ì™¸ë¶€ ê¸°ëŠ¥ì´ í•„ìš”í•  ë•Œì— í˜¸ì¶œë  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. ë„êµ¬ì˜ í˜¸ì¶œ ë°©ì‹ì„ ê·œê²©í™”í•˜ëŠ” Toolì´ ê°™ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n Callbacks:\\n\\n  LangChainì˜ ê° ë™ì‘ ë‹¨ê³„ë§ˆë‹¤ hookingì„ í•  ìˆ˜ ìˆë„ë¡ Callbackì„ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§, ë¡œê¹…ì´ë‚˜ ìŠ¤íŠ¸ë¦¬ë°ì— ê´€ë ¨ëœ ê¸°ëŠ¥ì´ê¸° ë•Œë¬¸ì— ì´ ê¸€ì—ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šì„ ì˜ˆì •ì…ë‹ˆë‹¤.\\n\\n# ì˜ˆì œ: ë‹¨ìˆœí•œ ì±—ë´‡\\n\\n## ë‹¨ìˆœ ì§ˆë¬¸-ì‘ë‹µê¸°\\n\\nìš°ì„  í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê°€ì§œ LLMì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ê³µì‹ ë¬¸ì„œì—ì„œ Custom LLMì„ ì‘ì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.',\n",
       "  'score': 0.70750624,\n",
       "  'raw_content': '# LangChainì— ëŒ€í•˜ì—¬\\n\\nì˜¤ì§„ê· (Oh Jinkyun)\\n\\nJune 23, 2023\\n\\n* [ê°œìš”](#ê°œìš”)\\n* [í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œ](#í•´ê²°í•˜ë ¤ëŠ”-ë¬¸ì œ)\\n* [ëª¨ë“ˆ](#ëª¨ë“ˆ)\\n* [ì˜ˆì œ: ë‹¨ìˆœí•œ ì±—ë´‡](#ì˜ˆì œ-ë‹¨ìˆœí•œ-ì±—ë´‡)\\n  + [ë‹¨ìˆœ ì§ˆë¬¸-ì‘ë‹µê¸°](#ë‹¨ìˆœ-ì§ˆë¬¸-ì‘ë‹µê¸°)\\n  + [ëŒ€í™” ë§¥ë½ ê¸°ì–µí•˜ê¸°](#ëŒ€í™”-ë§¥ë½-ê¸°ì–µí•˜ê¸°)\\n  + [ê¸¸ì´ ì¤„ì´ê¸°](#ê¸¸ì´-ì¤„ì´ê¸°)\\n* [ì˜ˆì œ: Agent](#ì˜ˆì œ-agent)\\n* [ì˜ˆì œ: ì‹¤ì œ LLM(OpenAI)ìœ¼ë¡œ Agent ì‹¤í–‰](#ì˜ˆì œ-ì‹¤ì œ-llmopenaiìœ¼ë¡œ-agent-ì‹¤í–‰)\\n* [ê²°ë¡ ](#ê²°ë¡ )\\n* [References](#references)\\n\\n# ê°œìš”\\n\\nOpenAIì˜ ChatGPT ì´ë˜ë¡œ LLM(Large Language Model)ì€ AI, NLP ê´€ë ¨ìë“¤ ë¿ë§Œ ì•„ë‹ˆë¼ ì‚¬íšŒ ì „ë°˜ì ì¸ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n[LangChain](https://docs.langchain.com/docs/)ì€ ì´ëŸ° LLMì„ ì¢€ ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°œë…ë“¤ì„ ì¶”ìƒí™”í•˜ì—¬, LLMì„ ì‚¬ìš©í•˜ë©´ì„œ í¸ë¦¬í• ë§Œí•œ íŒ¨í„´ë“¤ì„ ê·œê²©í™”ì‹œí‚¨ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\\n\\nì´ ê¸€ì—ì„œëŠ” LangChainì˜ ì£¼ìš” ëª¨ë“ˆì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ê³¼, LangChainì´ ì–´ë–¤ í¸ë¦¬í•¨ì„ ì œê³µí•˜ê³  ì–´ë–»ê²Œ í™œìš©í•´ë³¼ ìˆ˜ ìˆì„ì§€ ì˜ˆì œë¥¼ í†µí•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\\n\\nLangChainì€ [Javascript ë²„ì „](https://js.langchain.com/docs/)ê³¼ [Python](https://python.langchain.com/en/latest/) ë²„ì „ì„ ì œê³µí•˜ëŠ”ë°, ì´ ê¸€ì€ ì´ ë‘˜ ì¤‘ì—ì„œ ì£¼ë¡œ Pythonì„ ì‚¬ìš©í•˜ì—¬ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\\n\\n# í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œ\\n\\nOpenAIì˜ ChatGPT, í˜¹ì€ Bingì˜ BingChat ê°™ì€ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•´ë³´ë©´ ì±„íŒ…ë§Œ ì…ë ¥í•˜ë©´ ë°”ë¡œ ë´‡ê³¼ ì±„íŒ…í•˜ëŠ” ê²ƒ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í¸ë¦¬í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ë’¤ì— ìˆëŠ” ì¤‘ì¶”ì  ê¸°ëŠ¥ì¸ LLMì€ ì–´ë–¤ ê¸€ì— ëŒ€í•œ ë‹µì´ ë ë§Œí•œ ê¸€ì„ ìƒì„±í•  ë¿ì´ì§€ ì›¹í˜ì´ì§€ì— ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í‘œì‹œë˜ê¸° ìœ„í•œ ì—¬ëŸ¬ê°€ì§€ ë°‘ì‘ì—…ê¹Œì§€ í•´ê²°í•´ì£¼ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.\\n\\nLLMì„ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ëŒ€ëµ ë‚˜ì—´í•´ë³´ìë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\\n\\n1. ì±—ë´‡ì˜ ì •ì±…ì„ ì„¸ìš°ê¸° ìœ„í•´ ìˆ¨ê²¨ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì •í•˜ê³  ë§¤ í˜¸ì¶œë§ˆë‹¤ LLMì´ í•´ë‹¹ ë‚´ìš©ì„ ìµœì´ˆì— ì œê³µë°›ì„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\\n2. ëŒ€í™”ì˜ ë§¥ë½ì„ ê¸°ì–µí•©ë‹ˆë‹¤.\\n3. ëŒ€í™”ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì• ë¶€ë¶„ì„ ìë¥´ê±°ë‚˜ ìš”ì•½í•˜ëŠ” ë“± LLMì´ ì²˜ë¦¬í•˜ê¸° ìš©ì´í•œ ì‚¬ì´ì¦ˆë¥¼ ìœ ì§€í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\\n4. LLMì˜ ë‹µë³€ì—ì„œ ì¸í„°ë„·ì— ì ‘ì†í•˜ì—¬ ë‚´ìš©ì„ ë³´ì¶©í•  ìˆ˜ ìˆì„ë§Œí•œ ë¶€ë¶„ì„ ì¸í„°ë„·ì— ì ‘ì†í•˜ì—¬ ë³´ì¶©í•©ë‹ˆë‹¤. (Bingì˜ BingChatì´ë‚˜ Googleì˜ Bardì˜ ê¸°ëŠ¥)\\n\\nì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ LangChainì€ ì—¬ëŸ¬ í˜•íƒœì˜ ì¶”ìƒí™”ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ê³ , ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µë˜ëŠ” êµ¬í˜„ì„ ì‚¬ìš©í•˜ê±°ë‚˜ í˜¹ì€ ìê¸°ê°€ ì›í•˜ëŠ” ì •ì±…ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ì—°ì‡„ì ì¸ ì²´ì¸(Chain)ì˜ í•œ ë¶€ë¶„ìœ¼ë¡œ ë™ì‘í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ LangChainì—ëŠ” ì±—ë´‡ ë¿ë§Œì´ ì•„ë‹ˆë¼, ì—°ì‡„ì ì¸ ë™ì‘ì´ í•„ìš”í•œ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë„ í•´ê²°í•˜ë ¤ê³  í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë°ì— ì£¼ì¶•ì´ ë˜ëŠ” ì¤‘ìš”í•œ ëª¨ë“ˆë“¤ì— ëŒ€í•´ ìš°ì„  ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\\n\\n# ëª¨ë“ˆ\\n\\n[LangChainì˜ ë¬¸ì„œ](https://python.langchain.com/en/latest/index.html)ì˜ ëª©ì°¨ì—ì„œëŠ” ê°€ì¥ ëŒ€í‘œì ì¸ ê²ƒìœ¼ë¡œ ì•„ë˜ì˜ ëª¨ë“ˆë“¤ì„ ëª…ì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nê° ëª¨ë“ˆë“¤ì˜ ì¢…ë¥˜ì™€ ê°„ëµí•œ ì„¤ëª…ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\\n\\n* Models:\\n\\n  LLMì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë„£ìœ¼ë©´ ë‹µë³€ì„ í•˜ê±°ë‚˜, ë¯¸ì™„ì„± ëœ ë¬¸ìì—´ì„ ë„£ìœ¼ë©´ ì™„ì„±ì„ í•˜ëŠ” ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. LLMê³¼ ChatModelì´ë¼ëŠ” í´ë˜ìŠ¤ê°€ ìˆëŠ”ë°, 2023ë…„ 6ì›” í˜„ì¬ ChatModelì˜ APIê°€ í™•ì •ë˜ì§€ ì•Šì•„ì„œ ì•„ì§ Custom ChatModelì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì œê³µë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” LLM í´ë˜ìŠ¤ë§Œì„ ì–¸ê¸‰í•˜ê² ìŠµë‹ˆë‹¤.\\n* Prompts:\\n\\n  ë§ê·¸ëŒ€ë¡œ LLMì— ë“¤ì–´ê°ˆ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ìƒí™”ì‹œí‚¨ í…œí”Œë¦¿ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë…¸ì¶œí•˜ì§€ ì•ŠëŠ” ìˆ¨ê²¨ì§„ í”„ë¡¬í”„íŠ¸ë‚˜ í›„ì— ì–¸ê¸‰í•  Memory, í˜¹ì€ ì±„íŒ…ì˜ ê²½ìš° ë©”ì‹œì§€ ëª©ë¡ì„ LLMì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìì—´ì„ ì˜ ì¡°ë¦½í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\\n* Memory:\\n\\n  ì±„íŒ… ê¸°ë¡ì´ë‚˜ íŠ¹ì • ì‹œì ì—ì„œ ì´ì „ì˜ ìƒí˜¸ì‘ìš©ì„ ê¸°ì–µí•˜ê¸° ìœ„í•œ ì €ì¥ì†Œì™€ ê°™ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì…ë ¥ê³¼ ì¶œë ¥ì˜ Historyë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.\\n* Indexes:\\n\\n  LLMì´ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë¬¸ì„œì— ì ‘ê·¼í•˜ëŠ” í‘œì¤€ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼í…Œë©´ Retrieversê°€ ìˆëŠ”ë°, ì–´ë–¤ ë¬¸ìì—´ì„ ì£¼ê³  ê·¸ ë¬¸ìì—´ê³¼ ê´€ë ¨ëœ Document(ì´ ëª©ë¡ì—ëŠ” ì—†ì§€ë§Œ Document ë˜í•œ LangChainì—ì„œ ì •ì˜í•´ë†“ì€ í´ë˜ìŠ¤ì…ë‹ˆë‹¤)ì˜ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ëª¨ë“ˆì— ëŒ€í•´ì„œëŠ” ì´ ê¸€ì—ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šê² ìŠµë‹ˆë‹¤.\\n* Chains:\\n\\n  ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì„ ë¬¶ì–´ì„œ ê°™ì´ ë§ë¬¼ë ¤ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ì¤‘ì‹¬ì ì¸ ëª¨ë“ˆì…ë‹ˆë‹¤. ì²´ì¸ì„ ìƒì„±í•˜ë©´ì„œ í•´ë‹¹ ì²´ì¸ì— ì–´ë–¤ Modelì„ ì‚¬ìš©í•˜ê³ , ì–´ë–¤ Promptë¥¼ ì‚¬ìš©í•˜ê³ , ì–´ë–¤ Memoryë¥¼ ì‚¬ìš©í• ì§€ ë“±ë“±ì„ ê²°ì •í•©ë‹ˆë‹¤.\\n* Agents:\\n\\n  LLMì´ ì§ì ‘ ë‹µí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì™¸ë¶€ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•˜ê±°ë‚˜ ìœ ìš©í•œ ê²ƒë“¤, ì´ë¥¼í…Œë©´ ìˆ˜í•™ì ì¸ ê³„ì‚°ì´ë‚˜ ì˜ êµ¬ì¡°í™” ëœ ë°ì´í„°ì˜ ì¡°íšŒ(ì˜ˆì‹œ: íŠ¹ì • ë°°ìš°ì˜ ë‚˜ì´, ì¶œì—°ì‘ì„ ì¡°íšŒí•˜ê³  ì‹¶ë‹¤)ê°™ì€ LLM ì™¸ë¶€ ê¸°ëŠ¥ì´ í•„ìš”í•  ë•Œì— í˜¸ì¶œë  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. ë„êµ¬ì˜ í˜¸ì¶œ ë°©ì‹ì„ ê·œê²©í™”í•˜ëŠ” Toolì´ ê°™ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n* Callbacks:\\n\\n  LangChainì˜ ê° ë™ì‘ ë‹¨ê³„ë§ˆë‹¤ hookingì„ í•  ìˆ˜ ìˆë„ë¡ Callbackì„ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§, ë¡œê¹…ì´ë‚˜ ìŠ¤íŠ¸ë¦¬ë°ì— ê´€ë ¨ëœ ê¸°ëŠ¥ì´ê¸° ë•Œë¬¸ì— ì´ ê¸€ì—ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šì„ ì˜ˆì •ì…ë‹ˆë‹¤.\\n\\n# ì˜ˆì œ: ë‹¨ìˆœí•œ ì±—ë´‡\\n\\n## ë‹¨ìˆœ ì§ˆë¬¸-ì‘ë‹µê¸°\\n\\nìš°ì„  í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê°€ì§œ LLMì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ê³µì‹ ë¬¸ì„œì—ì„œ [Custom LLMì„ ì‘ì„±í•˜ëŠ” ë°©ë²•](https://python.langchain.com/en/latest/modules/models/llms/examples/custom_llm.html)ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nCustom LLMì˜ ìµœì†Œí•œì˜ êµ¬í˜„ì€ `_call` í•¨ìˆ˜ë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤. ì‹¤ì œ LLMì„ í†µí•œ êµ¬í˜„ì´ë¼ë©´ `_call` í•¨ìˆ˜ì˜ ì¸ìë¡œ ë°›ì€ promptë¥¼ LLMì— ë„˜ê¸°ëŠ” ë“±ì˜ ë™ì‘ì„ í•˜ê² ì§€ë§Œ, ì´ ì˜ˆì œì—ì„œëŠ” ë¬´ì¡°ê±´ ì´ˆê¸°í™” í•  ë•Œ `reply`ë¡œ ì£¼ì–´ì§„ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ë„ë¡ Custom LLMì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\\n\\n```\\n# custom_llm.py from typing import Any, List, Mapping, Optional fromlangchain.callbacks.manager import CallbackManagerForLLMRun fromlangchain.llms.base import LLM class CustomLLM(LLM): reply: str @ property def _llm_type(self) -> str: return \"custom\" def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None,) -> str: return self. reply @ property def _identifying_params(self) -> Mapping[str, Any]:\"\"\"Get the identifying parameters.\"\"\" return{\"reply\": self. reply}\\n```\\n\\nìœ„ì˜ ê°€ì§œ LLM í´ë˜ìŠ¤ë¥¼ í…ŒìŠ¤íŠ¸í•´ ë³´ë ¤ë©´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“  í›„ ë°”ë¡œ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.\\n\\n```\\nllm = CustomLLM(reply =\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\') print(llm(\\'ë‹¹ì‹ ì€ ì–´ë–¤ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?\\'))\\n```\\n\\n*ì¶œë ¥:*\\n\\n```\\në‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. \\n```\\n\\nì´ LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê°„ë‹¨í•œ Chainë¶€í„° ì ì  ê¸°ëŠ¥ì„ ë”í•˜ë©´ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ìš°ì„ ì€ ê°€ì¥ ê°„ë‹¨í•œ, í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì§ˆ ë¿ì¸ ì§ˆë¬¸/ë‹µë³€ ê¸°ëŠ¥ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.\\n\\n```\\n# app/ex01_basic.py from langchain import PromptTemplate, LLMChain fromapp.custom_llm import CustomLLM llm = CustomLLM(reply =\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\') template =\"\"\" ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: {question} \"\"\" prompt = PromptTemplate(input_variables =[\"question\"], template = template,) chain = LLMChain(llm = llm, prompt = prompt, verbose = True) if __name__ == \\'__main__\\': print(chain. run(\\'ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\\'))\\n```\\n\\n*ì¶œë ¥:*\\n\\n```\\n> Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ? > Finished chain. ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. \\n```\\n\\nìœ„ì™€ ê°™ì´ PromptTemplateì™€ LLMChainì„ ì´ìš©í•˜ì—¬ chainì„ êµ¬ì„±í•´ ë†“ìœ¼ë©´ `chain.run` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì§ˆë¬¸ë§Œ ë°”ê¿”ê°€ë©° LLMì— ì—¬ëŸ¬ê°€ì§€ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. FastAPIë‚˜ Flask ê°™ì€ ì›¹ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„œë²„ë¡œ ë§Œë“¤ì–´ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\\n\\n```\\nfrom fastapi import FastAPI fromapp.ex01_basic import chain app = FastAPI() @ app. get(\\'/\\') def answer_with_llm(question): return chain. run(question)\\n```\\n\\nëª…ë ¹ì°½:\\n\\n```\\n$ --reload\\n```\\n\\n*â€œì •ë§ë¡œ ì–´ë–¤ ì§ˆë¬¸ì„ í•´ë„ ë˜‘ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆê¹Œ?â€ë¼ëŠ” ì§ˆë¬¸ì„ í•˜ë©´ â€œë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.â€ë¼ëŠ” ë‹µë³€ì´ ëŒì•„ì˜¤ëŠ” OpenAPI í…ŒìŠ¤íŠ¸ì˜ ì´ë¯¸ì§€*\\n\\n## ëŒ€í™” ë§¥ë½ ê¸°ì–µí•˜ê¸°\\n\\ní•˜ì§€ë§Œ ìœ„ì—ì„œ ë§Œë“  chainì€ ëŒ€í™”ì˜ ë§¥ë½ì„ ê¸°ì–µí•˜ì§€ ëª»í•˜ê³  í•˜ë‚˜ì˜ ì§ˆë¬¸ì—ë§Œ ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ChatGPT ê°™ì€ ì„œë¹„ìŠ¤ì²˜ëŸ¼ ëŒ€í™”ì˜ ë§¥ë½ì„ LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì§€ê¸ˆê¹Œì§€ í•´ì˜¨ ëª¨ë“  ëŒ€í™”ì˜ ë‚´ìš©ì„ LLMì— ì „ë‹¬í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì¡´ì¬í•˜ëŠ” ê²ƒì´ Memoryì…ë‹ˆë‹¤. Memoryì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” keyì™€ ë§ì¶”ê¸° ìœ„í•´ templateì„ ì¡°ê¸ˆ ë³€ê²½í•œ ë’¤, ì´ ëª¨ë‘ë¥¼ Chainì— ì—°ê²°í•˜ê² ìŠµë‹ˆë‹¤.\\n\\n```\\n# app/ex02_memory.py from pprint import pprint from langchain import PromptTemplate fromlangchain.memory import ConversationBufferMemory fromlangchain.chains import ConversationChain fromapp.custom_llm import CustomLLM llm = CustomLLM(reply =\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\') template =\"\"\" ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. {history} ì‚¬ëŒ: {input} \"\"\" prompt = PromptTemplate(input_variables =[\"history\", \"input\"], template = template,) memory = ConversationBufferMemory(human_prefix = \\'ì‚¬ëŒ\\', ai_prefix = \\'AI\\') chain = ConversationChain(llm = llm, prompt = prompt, memory = memory, verbose = True) chain. predict(input = \\'ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\\') chain. predict(input = \\'ì •ë§ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆê¹Œ?\\') chain. predict(input =\\'í•˜ì§€ë§Œ ì´ì „ì— í–ˆë˜ ëŒ€í™”ë“¤ì€ ê¸°ì–µí•˜ê³  ìˆìœ¼ë¦¬ë¼ ë¯¿ìŠµë‹ˆë‹¤.\\') pprint(memory. chat_memory. messages)\\n```\\n\\n*ì¶œë ¥:*\\n\\n```\\n> Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ? > Finished chain. > Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ? AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì •ë§ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆê¹Œ? > Finished chain. > Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ? AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì •ë§ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆê¹Œ? AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: í•˜ì§€ë§Œ ì´ì „ì— í–ˆë˜ ëŒ€í™”ë“¤ì€ ê¸°ì–µí•˜ê³  ìˆìœ¼ë¦¬ë¼ ë¯¿ìŠµë‹ˆë‹¤. > Finished chain. [HumanMessage(content=\\'ë‹¹ì‹ ì€ ì–´ë–¤ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì •ë§ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆê¹Œ?\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'í•˜ì§€ë§Œ ì´ì „ì— í–ˆë˜ ëŒ€í™”ë“¤ì€ ê¸°ì–µí•˜ê³  ìˆìœ¼ë¦¬ë¼ ë¯¿ìŠµë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False)] \\n```\\n\\nConversationBufferMemoryëŠ” Messageì˜ ë¦¬ìŠ¤íŠ¸ì˜ í˜•íƒœë¡œ ì§€ë‚œ ì…ë ¥ê³¼ ì¶œë ¥ë“¤ì„ ê¸°ì–µí•˜ê³ , í”„ë¡¬í”„íŠ¸ì˜ historyì—ëŠ” ê¸°ì¡´ ëŒ€í™”ë“¤ì„, inputì—ëŠ” ì´ë²ˆ ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ê°€ê²Œ í•´ì¤ë‹ˆë‹¤. ConversationChainì´ ì´ëŸ¬í•œ ê³¼ì •ì„ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.\\n\\nChain ìƒì„±ì‹œì˜ `verbose=True` ì¸ì ë•Œë¬¸ì— `chain.predict`ë¥¼ ì„¸ ë²ˆ í˜¸ì¶œí•˜ë©´ì„œ `{history}`ì— ê¸°ì¡´ ëŒ€í™”ì˜ ë‚´ìš©ë“¤ì´ ëˆ„ì ë˜ì–´ ì…ë ¥ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ `memory.chat_memory.messages`ë¥¼ ì¶œë ¥í•´ë³´ë©´ `HumanMessage`ì™€ `AIMessage`ê°€ ë²ˆê°ˆì•„ê°€ë©° ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ê¸°ì–µë˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ì›¹ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ì‚¬ìš©ìë¥¼ ì‹ë³„í•˜ì—¬ ì ‘ì† ì¢…ë£Œì‹œ Historyë¥¼ DBì— ë³´ì¡´í•˜ê³  ì¬ì ‘ì†ì‹œ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ ì´ˆê¸°í™”í•˜ëŠ” ì¶”ê°€ì ì¸ ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.\\n\\nê·¸ëŸ° ì²˜ë¦¬ë¥¼ ì‰½ê²Œ í•˜ê¸° ìœ„í•˜ì—¬ íˆìŠ¤í† ë¦¬ë¥¼ `dict`ë¡œ ë³€í™˜í•˜ê±°ë‚˜ `dict`ì—ì„œ íˆìŠ¤í† ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ [ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©](https://python.langchain.com/en/latest/modules/memory/getting_started.html#saving-message-history)ì´ ìˆìŠµë‹ˆë‹¤.\\n\\n## ê¸¸ì´ ì¤„ì´ê¸°\\n\\nì±—ë´‡ ê°™ì€ ì„œë¹„ìŠ¤ì—ì„œ LLMê³¼ ë‚˜ëˆ„ëŠ” ëŒ€í™”ê°€ ê¸¸ì–´ì¡Œì„ ë•Œ, LLMì— ëª¨ë“  ëŒ€í™”ê°€ ë“¤ì–´ê°€ë©´ ë¬¼ë¡  ì´ìƒì ì´ê² ì§€ë§Œ ì„±ëŠ¥ì´ë‚˜ ë¹„ìš©ì˜ ë¬¸ì œë¡œ ì…ë ¥ì˜ ê¸¸ì´ì— ì–´ëŠì •ë„ ì œí•œì„ ë‘ëŠ” ê²ƒì´ í˜„ì‹¤ì ì…ë‹ˆë‹¤.\\n\\nì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ì¸ê¸° ìˆëŠ” ì „ëµì˜ ë‘ê°€ì§€ ì¤‘ í•˜ë‚˜ëŠ” ê°€ì¥ ìµœê·¼ì˜ ëª‡ ê°œ ëŒ€í™”ë§Œì„ LLMì— ë„˜ê²¨ì£¼ëŠ” ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì•ì˜ ë‚´ìš©ì„ LLMì„ í†µí•˜ì—¬ ìš”ì•½í•˜ëŠ” ë°©ë²•ì´ ìˆê² ìŠµë‹ˆë‹¤.\\n\\në‘ ê°€ì§€ ì „ëµ ëª¨ë‘ LangChainì— êµ¬í˜„ì²´ê°€ ìˆëŠ”ë°, ì „ìëŠ” `ConversationBufferWindowMemory`ë¼ëŠ” í´ë˜ìŠ¤ë¡œ êµ¬í˜„ë˜ì–´ ìˆê³ , í›„ìëŠ” `ConversationSummaryMemory`ë‚˜ `ConversationSummaryBufferMemory`ê°€ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ì™¸ì—ë„ ë‹¤ë¥¸ ì „ëµì´ë‚˜ íˆìŠ¤í† ë¦¬ì˜ ì €ì¥ê³¼ ë³µêµ¬ì— ê´€ë ¨ëœ êµ¬í˜„ì²´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. Memoryì— ê´€í•œ [How-To Guides](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html)ì— ê´€ë ¨ ë‚´ìš©ì´ ì„œìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ ê¸€ì—ì„œëŠ” ê°€ì¥ ê°„ë‹¨í•œ `ConversationBufferWindowMemory`ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\\n\\n```\\n# app/ex03_memwindow.py from pprint import pprint from langchain import PromptTemplate fromlangchain.memory import ConversationBufferWindowMemory fromlangchain.chains import ConversationChain fromapp.custom_llm import CustomLLM llm = CustomLLM(reply =\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\') template =\"\"\" ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. {history} ì‚¬ëŒ: {input} \"\"\" prompt = PromptTemplate(input_variables =[\"history\", \"input\"], template = template,) memory = ConversationBufferWindowMemory(human_prefix = \\'ì‚¬ëŒ\\', ai_prefix = \\'AI\\', k = 3) chain = ConversationChain(llm = llm, prompt = prompt, memory = memory, verbose = True) chain. predict(input =\\'ë‹¹ì‹ ì˜ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\\') for idx in range(10): chain. predict(input = f \\'ì´ê²ƒì€ {idx}ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\') chain. predict(input = \"ì´ì œ ëª‡ ë²ˆì§¸ ì „ ëŒ€í™”ê¹Œì§€ ê¸°ì–µí•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì£¼ì„¸ìš”\") pprint(memory. chat_memory. messages)\\n```\\n\\n*ì¶œë ¥:*\\n\\n```\\n> Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ë‹¹ì‹ ì˜ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ê³  í•©ë‹ˆë‹¤. > Finished chain. > Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ë‹¹ì‹ ì˜ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ê³  í•©ë‹ˆë‹¤. AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì´ê²ƒì€ 0ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤. > Finished chain. <...ì¤‘ëµ...> > Finished chain. > Entering new chain... Prompt after formatting: ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ë¡œì„œ ì§ˆì˜ì \\'ì‚¬ëŒ\\'ì´ í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì„±ì‹¤í•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìŠ¤ìŠ¤ë¡œê°€ ê°€ì§€ì§€ ì•Šì€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë°›ì•˜ì„ ë•Œì—ëŠ” ìì‹ ì˜ í•œê³„ì— ëŒ€í•´ì„œ ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì´ê²ƒì€ 7ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤. AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì´ê²ƒì€ 8ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤. AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì´ê²ƒì€ 9ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤. AI: ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤. ì‚¬ëŒ: ì´ì œ ëª‡ ë²ˆì§¸ ì „ ëŒ€í™”ê¹Œì§€ ê¸°ì–µí•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì£¼ì„¸ìš” > Finished chain. [HumanMessage(content=\\'ë‹¹ì‹ ì˜ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 0ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 1ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 2ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 3ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 4ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 5ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 6ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 7ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 8ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ê²ƒì€ 9ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False), HumanMessage(content=\\'ì´ì œ ëª‡ ë²ˆì§¸ ì „ ëŒ€í™”ê¹Œì§€ ê¸°ì–µí•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì£¼ì„¸ìš”\\', additional_kwargs={}, example=False), AIMessage(content=\\'ë‚˜ëŠ” ì œëŒ€ë¡œ ëœ LLMì´ ì•„ë‹ˆë¼ì„œ í•­ìƒ ê°™ì€ ëŒ€ë‹µë§Œ í•©ë‹ˆë‹¤.\\', additional_kwargs={}, example=False)] \\n```\\n\\nìœ„ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ë©´ í”„ë¡¬í”„íŠ¸ì—ëŠ” Memoryë¥¼ ìƒì„±í•  ë•Œ ì£¼ì–´ì§„ `k=3` ì¸ìì— ì˜í•´ ê°€ì¥ ìµœê·¼ì˜ ì„¸ ë²ˆì˜ ëŒ€í™”ë§Œ ì…ë ¥ë˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\ní•˜ì§€ë§Œ LLMì—ê²Œ ì œê³µí•˜ëŠ” ê²ƒì´ ë§ˆì§€ë§‰ì˜ `k`ê°œì¼ ë¿, `pprint`ì˜ ì¶œë ¥ ë‚´ìš©ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼ History ê°ì²´ëŠ” ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ ë³´ê´€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì›¹ í™”ë©´ ê°™ì€ ê³³ì—ì„œ ì‚¬ìš©ìê°€ ë³¼ ì±„íŒ… ë‚´ì—­ê³¼ ìƒíƒœê°€ ì¼ì¹˜í•˜ë„ë¡ í•˜ëŠ” ë° ìœ ìš©íˆ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nì§€ê¸ˆê¹Œì§€ ì´ì•¼ê¸°í•œ ê¸°ëŠ¥ë“¤ê³¼ ë”ë¶ˆì–´, íˆìŠ¤í† ë¦¬ ì €ì¥ê³¼ ë³µêµ¬ì— ê´€ë ¨ëœ `MongoDBChatMessageHistory` í´ë˜ìŠ¤ ë“±ì„ ì¶”ê°€ë¡œ í™œìš©í•œë‹¤ë©´ ChatGPTì™€ ë¹„ìŠ·í•œ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë°ì— í•„ìš”í•œ ë„êµ¬ë“¤ì´ ëª¨ë‘ Langchainì— ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n# ì˜ˆì œ: Agent\\n\\n`AgentExecutor` ëª¨ë“ˆì€ íŠ¹ì •í•œ í˜•íƒœë¡œ ì¡°ë¦½ëœ Chainìœ¼ë¡œ, ì´ë¯¸ íŠ¹ì •í•œ í˜•ì‹ì˜ ìˆ¨ê²¨ì§„ Promptë¥¼ ê°–ê³  ìˆê³ , LLMì˜ ë‹µë³€ì„ íŒŒì‹±í•˜ê³ , ê·¸ íŒŒì‹±í•œ ê²°ê³¼ë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ì¶”ê°€ ì…ë ¥ ì—†ì´ë„ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜ ë‹¤ì‹œ LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µì„ ë„ì¶œí•´ëƒ…ë‹ˆë‹¤.\\n\\nì•„ë˜ì˜ ì½”ë“œëŠ” ìˆœì„œëŒ€ë¡œ ì •í•´ì§„ ë‹µì„ ë±‰ëŠ” ê°€ì§œ LLM êµ¬í˜„ì„ í™œìš©í•˜ì—¬ ë‘ ê°€ì§€ Toolì„ ì‹¤í–‰í•´ë³´ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n```\\n# app/ex04_agent.py fromlangchain.agents import load_tools, initialize_agent, AgentType fromlangchain.llms.base import LLM class AgentCustomLLM(LLM): replies: list[str] cursor = 0 @ property def _llm_type(self) -> str: return\"agent-custom\" def _call(self, prompt: str, stop = None, run_manager = None,) -> str: ret = self. replies[self. cursor] self. cursor += 1 return ret llm = AgentCustomLLM(replies =[\\'\\'\\'First, I need to run some python code. Action: Python REPL Action Input: foo = 42 bar = foo + 42 print(bar) \\'\\'\\',\\'\\'\\'Second, I need to show how requests tool works. Action: requests_get Action Input: https://example.com/ \\'\\'\\',\\'\\'\\'I\\'ve done what I was requested to do. Final Answer: Done \\'\\'\\',]) tools = load_tools([\\'python_repl\\', \\'requests\\']) agent = initialize_agent(tools, llm, agent = AgentType. ZERO_SHOT_REACT_DESCRIPTION, verbose = True) agent. run(\\'ì ë‹¹í•œ Python ì½”ë“œ ì‹¤í–‰ ì˜ˆì œì™€ requests ì‹¤í–‰ ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.\\')\\n```\\n\\n*ì¶œë ¥:*\\n\\n```\\n> Entering new chain... First, I need to run a python code. Action: Python REPL Action Input: foo = 42 bar = foo + 42 print(bar) Observation: 84 Thought: Second, I need to show how requests tool works. Action: requests_get Action Input: https://example.com/ Observation: \\n   Example Domain\\n\\n# Example Domain\\n\\n\\n\\nThis domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\\n\\n\\n\\nMore information...\\n\\nThought: I\\'ve done what I was requested to do. Final Answer: Done > Finished chain. \\n```\\n\\nAgentë¥¼ ì˜ ì„¤ì •í•˜ê³  ì¢‹ì€ ì§ˆë¬¸ê³¼ ì¢‹ì€ LLMë§Œ ìˆë‹¤ë©´ ì§ˆë¬¸ í•œ ë²ˆìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆì˜ Tool í˜¸ì¶œì„ ê±°ì³ AIê°€ ìŠ¤ìŠ¤ë¡œ ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì—¬ëŸ¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n`AgentExecutor`(`initialize_agent`ê°€ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì´ í´ë˜ìŠ¤ì…ë‹ˆë‹¤)ëŠ” ê° ë‹¨ê³„ë§ˆë‹¤ LLMì´ ë°˜í™˜í•˜ëŠ” ê°’ì„ íŒŒì‹±í•˜ì—¬ í•„ìš”í•œ ë„êµ¬ë“¤ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.\\n\\n`AgentExecutor`ê°€ ì›í•˜ëŠ” í˜•ì‹ì„ ì•Œë ¤ë©´ ìˆ¨ê²¨ì§„ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ë³´ë©´ ë©ë‹ˆë‹¤.\\n\\n`_call` í•¨ìˆ˜ì—ì„œ `prompt` ì¸ìë¥¼ ì¶œë ¥í•´ë³´ë©´ ê·¸ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\\n\\n```\\nAnswer the following questions as best you can. You have access to the following tools: Python REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`. requests_get: A portal to the internet. Use this when you need to get specific content from a website. Input should be a url (i.e. https://www.google.com). The output will be the text response of the GET request. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [Python REPL, requests_get] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: ì ë‹¹í•œ Python ì½”ë“œ ì‹¤í–‰ ì˜ˆì œì™€ requests ì‹¤í–‰ ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”. Thought: \\n```\\n\\nìˆ¨ê²¨ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•˜ë©´ AgentExecutorëŠ” Question, Thought, Action, Action Inputê³¼ ê°™ì´ ë‚˜ëˆ ì„œ ê¸°ìˆ í•´ì£¼ê¸°ë¥¼ ê¸°ëŒ€í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ LLMìœ¼ë¡œë¶€í„° ì´ í˜•ì‹ì— ë§ì§€ ì•ŠëŠ” ë¬¸ìì—´ì´ ëŒì•„ì˜¨ë‹¤ë©´ Agentê°€ íŒŒì‹± ì˜¤ë¥˜ë¥¼ ë‚´ë±‰ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì ì ˆí•œ ì˜ˆì™¸ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\\n\\nAgentExecutorëŠ” ì € í”„ë¡¬í”„íŠ¸ë¥¼ ì´ìš©í•´ Final Answerê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ê³„ì†í•´ì„œ LLMê³¼ Toolì„ ìë™ì ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤. ë”°ë¼ì„œ BingChatê³¼ ê°™ì´ ì¸í„°ë„· ìƒ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•˜ëŠ” ê¸°ëŠ¥ì„ ìì‹ ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì— êµ¬í˜„í•˜ê³  ì‹¶ë‹¤ë©´ Agentë¥¼ í™œìš©í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\\n\\n# ì˜ˆì œ: ì‹¤ì œ LLM(OpenAI)ìœ¼ë¡œ Agent ì‹¤í–‰\\n\\nì§€ê¸ˆê¹Œì§€ëŠ” í¸ì˜ë¥¼ ìœ„í•´ ì™„ë²½í•˜ê²Œ í†µì œí•  ìˆ˜ ìˆëŠ” ê°€ì§œ LLMë§Œì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤ë§Œ, ë§ˆì§€ë§‰ìœ¼ë¡œ OpenAIë¥¼ í†µí•´ ì‹¤ì œë¡œ ì˜ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\\n\\nì•„ë˜ì˜ ì½”ë“œëŠ” OpenAIê°€ ì£¼ì–´ì§„ íŒŒì´ì¬ ì½”ë“œë¥¼ ë³´ê³  ì§ì ‘ ì‹¤í–‰ê²°ê³¼ë¥¼ ìœ ì¶”í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, Agentì—ê²Œ ì ì ˆí•œ ë„êµ¬ë¥¼ ìš”ì²­í•´ì„œ ì •í™•í•œ ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°›ì•„ë³¸ ë’¤ ê·¸ ê²°ê³¼ë¥¼ í•´ì„í•˜ëŠ” í˜•íƒœë¡œ ì§„í–‰ë˜ê¸°ë¥¼ ê¸°ëŒ€í•˜ë©° ì‘ì„±í•œ ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n```\\nfromlangchain.llms import OpenAI fromlangchain.agents import load_tools, initialize_agent, AgentType llm = OpenAI() tools = load_tools([\"python_repl\"]) agent = initialize_agent(tools, llm, agent = AgentType. ZERO_SHOT_REACT_DESCRIPTION, verbose = True,) agent. run(\"\"\"What will be printed if I execute following python code? for i in range(5, 0, -1): print(\"*\" * i) \"\"\")\\n```\\n\\n*ì¶œë ¥:*\\n\\n```\\n> Entering new chain... I need to execute the code to see the output Action: Python REPL Action Input: for i in range(5, 0, -1): print(\"*\" * i) Observation: ***** **** *** ** * Thought: I now know the final answer Final Answer: Five lines of asterisks, each line with one fewer asterisk than the line before it. \\n```\\n\\nì²´ì¸ì˜ ì§„í–‰ê³¼ì •ì„ ì‚´í´ë³´ë©´ ì˜ë„ëŒ€ë¡œ ì˜ ë™ì‘í–ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ LLM ì²´ì¸ì˜ ì‘ë™ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\\n\\n1. Agentì˜ ìˆ¨ê²¨ì§„ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬, ê¸°ëŒ€í•˜ëŠ” ë‹µë³€ í˜•ì‹, ì§ˆë¬¸ì„ ì˜ ì‚½ì…í•´ì„œ OpenAIì— ìš”ì²­í•©ë‹ˆë‹¤.\\n2. OpenAIëŠ” `python_repl` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì ì ˆí•  ê²ƒ ê°™ê³  ê·¸ ë„êµ¬ì˜ ì¸ìë¡œ ì§ˆë¬¸ìê°€ ì…ë ¥í•œ ì½”ë“œë¥¼ ë„£ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ê³  ë‹µë³€í•©ë‹ˆë‹¤.\\n3. Agentê°€ OpenAIì˜ ë‹µë³€ì„ íŒŒì‹±í•˜ì—¬ ì‚¬ìš©í•˜ë ¤ëŠ” ë„êµ¬ì— ì¸ìë¥¼ ë„£ê³  ì‹¤í–‰í•´ ê·¸ ê²°ê³¼ë¥¼ Observationì— ë„£ì–´ì¤ë‹ˆë‹¤.\\n4. Observation ê²°ê³¼ê¹Œì§€ ë”í•´ì„œ ë‹¤ì‹œ OpenAIì— ìš”ì²­í•©ë‹ˆë‹¤.\\n5. OpenAIëŠ” ê¸°ì¡´ ì§ˆë¬¸ê³¼ Observationì˜ ë‚´ìš©ì„ ë³´ê³  ì¶©ë¶„íˆ ë‚´ìš©ì„ ì•Œ ìˆ˜ ìˆë‹¤ê³  íŒë‹¨í•˜ì˜€ëŠ”ì§€ Final Answerì— í”„ë¦°íŠ¸ ë˜ëŠ” ë‚´ìš©ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\\n6. AgentëŠ” Final Answerê°€ ìˆëŠ” ê²ƒì„ ì¸ì‹í•˜ì—¬ ì´ë¥¼ ë°˜í™˜í•˜ê³  ì²´ì¸ì´ ì¢…ë£Œë©ë‹ˆë‹¤.\\n\\nì•½ê°„ ì•„ì‰¬ìš´ ì ì´ ìˆë‹¤ë©´ ì˜ˆì œì—ì„œ ì‚¬ìš©í•œ `ZERO_SHOT_REACT_DESCRIPTION` AgentëŠ” ì‚¬ì‹¤ìƒ OpenAIë‚˜ ê·¸ì™€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì˜ ìƒì—…ì ì¸ LLMì´ ì•„ë‹ˆë©´ ë™ì‘ì‹œí‚¤ê¸°ê°€ ê½¤ í˜ë“¤ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. GPT2, llama, vicuna ë“±ì˜ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì— ì •ë°€í•œ ì„¤ì • ì—†ì´ ì‚¬ìš©í•  ê²½ìš°, Agentê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ì„ ì•½ê°„ì”© ì–´ê¸°ê±°ë‚˜, Observation(Action ì‹¤í–‰ì˜ ê²°ê³¼)ê¹Œì§€ ìë™ì™„ì„±í•´ë²„ë¦¬ëŠ” ì–´ì²˜êµ¬ë‹ˆ ì—†ëŠ” ê²½ìš°ê°€ ìƒê²¨ì„œ ìš´ì´ ì¢‹ì„ ë•Œì—ë§Œ ë¬¸ì œ ì—†ì´ ë™ì‘í•©ë‹ˆë‹¤.\\n\\n# ê²°ë¡ \\n\\nLangChainì€ LLMê³¼ LLMì„ í™œìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ë° í•„ìš”í•œ ì—¬ëŸ¬ê°€ì§€ ë””ìì¸ íŒ¨í„´ì„ í‘œì¤€í™”ì‹œí‚¤ë ¤ê³  ë…¸ë ¥í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. 2023ë…„ 6ì›” í˜„ì¬ ë²„ì „ë„ 0.0.200 ë²„ì „ìœ¼ë¡œ ì•„ì§ ì„±ìˆ™í•˜ì§€ ì•Šì•˜ê³ , ì´ ê¸€ì—ì„œë„ ì ê¹ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´ `ChatModel`ì²˜ëŸ¼ customize ê°€ëŠ¥í•˜ê²Œ í•  ì˜ˆì •ì´ ìˆì§€ë§Œ ì•„ì§ ê·¸ ê·œì¹™ì´ ì •í•´ì§€ì§€ ì•Šì€ ê²ƒë„ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¯¸ì„±ìˆ™í•œ ë‹¨ê³„ì„ì—ë„ ì‹¤ì œë¡œ ì“¸ ë²•í•œ ìš©ë¡€ë¥¼ ê½¤ ë§ì´ ê³ ë ¤í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì´ ë¬¸ì„œë‚˜ ì˜ˆì œì—ì„œ ì˜ ë³´ì…ë‹ˆë‹¤.\\n\\nê·¸ë¦¬ê³  (ë§Œì•½ ì˜ ë™ì‘í•œë‹¤ë©´) `Agent`ê°€ ëª…ë ¹ í•œ ë²ˆì— ì—¬ëŸ¬ Toolì„ ì˜¤ê°€ë©° ë§ˆë²•ì²˜ëŸ¼ ì—¬ëŸ¬ê°€ì§€ ë™ì‘ì„ í•˜ëŠ” ê²ƒì€ í„°ë¯¸ë„ ì°½ì— ì§€ë‚˜ê°€ëŠ” ì‹¤í–‰ ê³¼ì • í…ìŠ¤íŠ¸ë¥¼ ë³´ëŠ” ê²ƒë§Œìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì ì´ ìˆìŠµë‹ˆë‹¤.\\n\\nì—¬ëŸ¬ í•œê³„ì—ë„ ë¶ˆêµ¬í•˜ê³  LLMì„ í™œìš©í•˜ë©´ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ê°€ ìˆê±°ë‚˜, ë„êµ¬ë¥¼ í†µí•´ LLMì„ ë³´ì¡°í•˜ê³  ì‹¶ê±°ë‚˜, ë‹¨ê³„ì ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ LLMì„ í˜¸ì¶œí•´ì•¼ í•  í•„ìš”ê°€ ìˆë‹¤ë©´ LangChainì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì œë¥¼ ì •ë¦¬í•˜ëŠ” ê²ƒì€ ì„¤ê³„ì˜ êµí†µì •ë¦¬ì— ê½¤ ë„ì›€ì´ ë  ë“¯ í•©ë‹ˆë‹¤. LLMìœ¼ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‘ì„±í•˜ëŠ” ë°ì— Langchainì´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë¦¬ë¼ ìƒê°í•©ë‹ˆë‹¤.\\n\\n# References\\n\\n* <https://python.langchain.com/en/latest/index.html>\\n* <https://medium.com/databutton/getting-started-with-langchain-a-powerful-tool-for-working-with-large-language-models-286419ba0842>\\n\\n* [NLP](/ncresearch/tags#NLP)\\n\\nPrevious post\\n\\n [ë°ì´í„° í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ê°€ëª…ì •ë³´ ì´ìš© ë°©ë²•ê³¼ ì „ë¬¸ì ì¸ ì§€ì¹¨ì„œ](/ncresearch/b7ec86b60700d047cf4b548a1add5b294d68b23e)\\n\\nNext post\\n\\n [Cross Entropy, ìµœì„ ì¸ê°€ìš”?: ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ì—¬ëŸ¬ ì†ì‹¤ í•¨ìˆ˜ë“¤ 1](/ncresearch/481b04ee12af454ab1de6dde2576eb4c15f8047a)\\n\\n '},\n",
       " {'title': 'LangGraph overview - Docs by LangChain',\n",
       "  'url': 'https://langchain-ai.github.io/langgraphjs/tutorials/langgraph-platform/local-server/?q=',\n",
       "  'content': 'We will commonly use LangChain components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. [...] Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.## LangChain\\n\\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n## \\u200b Acknowledgements',\n",
       "  'score': 0.6565047,\n",
       "  'raw_content': '[Skip to main content](#content-area)\\n\\n* [Overview](/oss/javascript/langgraph/overview)\\n\\n##### LangGraph v1.0\\n\\n* [Release notes](/oss/javascript/releases/langgraph-v1)\\n* [Migration guide](/oss/javascript/migrate/langgraph-v1)\\n\\n##### Get started\\n\\n* [Install](/oss/javascript/langgraph/install)\\n* [Quickstart](/oss/javascript/langgraph/quickstart)\\n* [Local server](/oss/javascript/langgraph/local-server)\\n* [Thinking in LangGraph](/oss/javascript/langgraph/thinking-in-langgraph)\\n* [Workflows + agents](/oss/javascript/langgraph/workflows-agents)\\n\\n##### Capabilities\\n\\n* [Persistence](/oss/javascript/langgraph/persistence)\\n* [Durable execution](/oss/javascript/langgraph/durable-execution)\\n* [Streaming](/oss/javascript/langgraph/streaming)\\n* [Interrupts](/oss/javascript/langgraph/interrupts)\\n* [Time travel](/oss/javascript/langgraph/use-time-travel)\\n* [Memory](/oss/javascript/langgraph/add-memory)\\n* [Subgraphs](/oss/javascript/langgraph/use-subgraphs)\\n\\n##### Production\\n\\n* [Application structure](/oss/javascript/langgraph/application-structure)\\n* [Test](/oss/javascript/langgraph/test)\\n* [LangSmith Studio](/oss/javascript/langgraph/studio)\\n* [Agent Chat UI](/oss/javascript/langgraph/ui)\\n* [LangSmith Deployment](/oss/javascript/langgraph/deploy)\\n* [LangSmith Observability](/oss/javascript/langgraph/observability)\\n\\n##### LangGraph APIs\\n\\n* [Runtime](/oss/javascript/langgraph/pregel)\\n\\n* [Install](#install)\\n* [Core benefits](#core-benefits)\\n* [LangGraph ecosystem](#langgraph-ecosystem)\\n* [Acknowledgements](#acknowledgements)\\n\\n# LangGraph overview\\n\\n**LangGraph v1.0 is now available!**For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/javascript/releases/langgraph-v1) and [migration guide](/oss/javascript/migrate/langgraph-v1).If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml&labels=langgraph,js/ts) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraphjs/tree/main/docs/docs).\\n\\nTrusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\n## [\\u200b](#install) Install\\n\\nCopy\\n\\n```\\nnpm install @langchain/langgraph @langchain/core\\n\\n```\\n\\nThen, create a simple hello world example:\\n\\nCopy\\n\\n```\\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\\n\\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\\n};\\n\\nconst graph = new StateGraph(MessagesAnnotation)\\n  .addNode(\"mock_llm\", mockLlm)\\n  .addEdge(START, \"mock_llm\")\\n  .addEdge(\"mock_llm\", END)\\n  .compile();\\n\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\\n\\n```\\n\\n## [\\u200b](#core-benefits) Core benefits\\n\\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n## [\\u200b](#langgraph-ecosystem) LangGraph ecosystem\\n\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\n[## LangSmith\\n\\nTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.](http://www.langchain.com/langsmith)[## LangGraph\\n\\nDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.](/langsmith/agent-server)[## LangChain\\n\\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.](/oss/javascript/langchain/overview)\\n\\n## [\\u200b](#acknowledgements) Acknowledgements\\n\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\n\\n---\\n\\n[Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\\n\\n[Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\\n\\nWas this page helpful?\\n\\n[What\\'s new in LangGraph v1](/oss/javascript/releases/langgraph-v1)'},\n",
       " {'title': 'LangGraph overview',\n",
       "  'url': 'https://langchain-ai.github.io/langgraphjs/versions/?q=',\n",
       "  'content': 'We will commonly use LangChain components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.',\n",
       "  'score': 0.6461259,\n",
       "  'raw_content': '[Skip to main content](#content-area)\\n\\n* [Overview](/oss/javascript/langgraph/overview)\\n\\n##### LangGraph v1.0\\n\\n* [Release notes](/oss/javascript/releases/langgraph-v1)\\n* [Migration guide](/oss/javascript/migrate/langgraph-v1)\\n\\n##### Get started\\n\\n* [Install](/oss/javascript/langgraph/install)\\n* [Quickstart](/oss/javascript/langgraph/quickstart)\\n* [Local server](/oss/javascript/langgraph/local-server)\\n* [Thinking in LangGraph](/oss/javascript/langgraph/thinking-in-langgraph)\\n* [Workflows + agents](/oss/javascript/langgraph/workflows-agents)\\n\\n##### Capabilities\\n\\n* [Persistence](/oss/javascript/langgraph/persistence)\\n* [Durable execution](/oss/javascript/langgraph/durable-execution)\\n* [Streaming](/oss/javascript/langgraph/streaming)\\n* [Interrupts](/oss/javascript/langgraph/interrupts)\\n* [Time travel](/oss/javascript/langgraph/use-time-travel)\\n* [Memory](/oss/javascript/langgraph/add-memory)\\n* [Subgraphs](/oss/javascript/langgraph/use-subgraphs)\\n\\n##### Production\\n\\n* [Application structure](/oss/javascript/langgraph/application-structure)\\n* [Test](/oss/javascript/langgraph/test)\\n* [LangSmith Studio](/oss/javascript/langgraph/studio)\\n* [Agent Chat UI](/oss/javascript/langgraph/ui)\\n* [LangSmith Deployment](/oss/javascript/langgraph/deploy)\\n* [LangSmith Observability](/oss/javascript/langgraph/observability)\\n\\n##### LangGraph APIs\\n\\n* [Runtime](/oss/javascript/langgraph/pregel)\\n\\n* [Install](#install)\\n* [Core benefits](#core-benefits)\\n* [LangGraph ecosystem](#langgraph-ecosystem)\\n* [Acknowledgements](#acknowledgements)\\n\\n# LangGraph overview\\n\\n**LangGraph v1.0 is now available!**For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/javascript/releases/langgraph-v1) and [migration guide](/oss/javascript/migrate/langgraph-v1).If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml&labels=langgraph,js/ts) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraphjs/tree/main/docs/docs).\\n\\nTrusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\n## [\\u200b](#install) Install\\n\\nCopy\\n\\n```\\nnpm install @langchain/langgraph @langchain/core\\n\\n```\\n\\nThen, create a simple hello world example:\\n\\nCopy\\n\\n```\\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\\n\\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\\n};\\n\\nconst graph = new StateGraph(MessagesAnnotation)\\n  .addNode(\"mock_llm\", mockLlm)\\n  .addEdge(START, \"mock_llm\")\\n  .addEdge(\"mock_llm\", END)\\n  .compile();\\n\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\\n\\n```\\n\\n## [\\u200b](#core-benefits) Core benefits\\n\\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n## [\\u200b](#langgraph-ecosystem) LangGraph ecosystem\\n\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\n[## LangSmith\\n\\nTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.](http://www.langchain.com/langsmith)[## LangGraph\\n\\nDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.](/langsmith/agent-server)[## LangChain\\n\\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.](/oss/javascript/langchain/overview)\\n\\n## [\\u200b](#acknowledgements) Acknowledgements\\n\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\n\\n---\\n\\n[Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\\n\\n[Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\\n\\nWas this page helpful?\\n\\n[What\\'s new in LangGraph v1](/oss/javascript/releases/langgraph-v1)'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë„êµ¬ ì‹¤í–‰\n",
    "tool.invoke({\"query\": \"LangChain Tools ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850afa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A neo-classical painting depicting a group of diverse people intently gazing at their smartphones, juxtaposed against a classical architectural backdrop. The scene is infused with irony, highlighting the contrast between the modern technology of smartphones and the grandeur of classical sculptures and columns. The individuals, dressed in a mix of contemporary and historical attire, exhibit a range of emotionsâ€”curiosity, distraction, and contemplation. Soft, natural lighting enhances the textures of their clothing and the details of the architecture, while the color palette features rich earth tones and muted pastels. The composition should evoke a sense of timelessness, blending modernity with classic elegance, while subtly critiquing society's obsession with technology.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9, max_tokens=1000)\n",
    "\n",
    "# DALL-E ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Generate a detailed IMAGE GENERATION prompt for DALL-E based on the following description. \"\n",
    "    \"Return only the prompt, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the prompt\"\n",
    "    \"Output should be less than 1000 characters. Write in English only.\"\n",
    "    \"Image Description: \\n{image_desc}\",\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸, LLM, ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„±\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "image_prompt = chain.invoke(\n",
    "    {\"image_desc\": \"ìŠ¤ë§ˆíŠ¸í°ì„ ë°”ë¼ë³´ëŠ” ì‚¬ëŒë“¤ì„ í’ìí•œ neo-classicism painting\"}\n",
    ")\n",
    "\n",
    "# ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì¶œë ¥\n",
    "print(image_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2f6329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-4OoS3Bbf80mYdwqULXrqx6gY/user-naZ9ctXptQxH3jwRMstAGSax/img-awG3793th6G34YwC9tQmAUkO.png?st=2025-11-28T03%3A19%3A42Z&se=2025-11-28T05%3A19%3A42Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=cc612491-d948-4d2e-9821-2683df3719f5&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-11-28T00%3A37%3A19Z&ske=2025-11-29T00%3A37%3A19Z&sks=b&skv=2024-08-04&sig=RKt82RAH2XLSTSEWvLgAXJpm/XHI5yhew1aWcaEKtdA%3D\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DALL-E API ë˜í¼ ê°€ì ¸ì˜¤ê¸°\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from IPython.display import Image\n",
    "import os\n",
    "\n",
    "# DALL-E API ë˜í¼ ì´ˆê¸°í™”\n",
    "# model: ì‚¬ìš©í•  DALL-E ëª¨ë¸ ë²„ì „\n",
    "# size: ìƒì„±í•  ì´ë¯¸ì§€ í¬ê¸°\n",
    "# quality: ì´ë¯¸ì§€ í’ˆì§ˆ\n",
    "# n: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜\n",
    "dalle = DallEAPIWrapper(\n",
    "    model=\"dall-e-3\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "# ì§ˆë¬¸\n",
    "query = \"ì§€í•˜ì² ì´ ì˜¤ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¶œê·¼ê¸¸ì˜ í•œêµ­ì¸ë“¤\"\n",
    "\n",
    "# ì´ë¯¸ì§€ ìƒì„± ë° URL ë°›ê¸°\n",
    "# chain.invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì„¤ëª…ì„ DALL-E í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\n",
    "# dalle.run()ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±\n",
    "image_url = dalle.run(chain.invoke({\"image_desc\": query}))\n",
    "\n",
    "# ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "Image(url=image_url, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d65e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "# ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61949c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë„êµ¬ ì‹¤í–‰\n",
    "add_numbers.invoke({\"a\": 3, \"b\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c366097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë„êµ¬ ì‹¤í–‰\n",
    "multiply_numbers.invoke({\"a\": 3, \"b\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08041d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
